---
title: "Using DeclareDesign to simulate GraphoGame data"
author: "Dorothy Bishop & Charles Hulme"
date: "18 Feb 2024"
output:
  html_document:
    df_print: paged
  reference_docx: mystyle.docx
  word_document: default
  pdf_document: default
editor_options:
  markdown:
    wrap: 72
    always_allow_html: yes
---

# Background  

Ahmed et al. (2020) conducted a reanalysis of data collected in a Randomized Controlled Trial of GraphoGame Rime, a computerized game for training English phonics knowledge in poor readers. The original research was funded by the Education Endowment Foundation and Wellcome Trust, and involved 398 Year 2 pupils in 15 primary schools. Ahmed et al. (2020) concluded that "The current study suggests that young learners of the English orthography show significant benefits in learning both phonic decoding skills and spelling skills from the supplementary use of GG Rime in addition to ongoing classroom literacy instruction."  This statement stands in sharp contrast to the original report on outcomes from the RCT, published in a report by the evaluator, the National Foundation for Educational Research, who concluded: "This study provided no evidence that GraphoGame Rime was effective at improving reading outcomes over and above business-as-usual, and this was a highly secure result." (See: https://www.nfer.ac.uk/publications/graphogame-rime-evaluation-report-and-executive-summary/).

<!---see also registration on http://www.isrctn.com/ISRCTN10467450-->

Those considering using Graphogame will find it confusing that such diametrically opposed conclusions can be drawn from analysis of the same dataset. We argue here that the conclusions from the analysis by Ahmed et al are insecure, because they used methods that introduce bias into estimates of effects of intervention - selection of subgroups for analysis after the study is completed.  The problems created by such methods are well-known in fields such as clinical trials and political science. For instance, in their Guideline on the investigation of subgroups in confirmatory clinical trials, the Committee for Medicinal Products for Human Use (2019) stated: "From a formal statistical point of view, no further confirmatory conclusions are possible in a clinical trial where the primary null hypothesis cannot be rejected."  And in a paper entitled: "How conditioning on post-treatment variables can ruin
your experiment and what to do about it", Montgomery et al (2018) noted the dangers of practices such as "dropping participants who fail manipulation checks; controlling for
variables measured after the treatment such as potential mediators; or subsetting samples
based on post-treatment variables". All of these practices can lead to biased estimates. Unfortunately, the analyses conducted by Ahmed et al (2020) fall in this category. Here we use simulations to show how, under certain reasonable assumptions, the methods that they used are likely to have inflated the estimate of the intervention effect. 

Simulations were conducted using the DeclareDesign package (Blair, Coppock, & Humphreys, 2023) in the R computing language (R Core Team, 2023). This package is designed to facilitate comparison of different research designs in terms of properties such as bias and statistical power. We used it to simulate datasets with the characteristics of the results from the original RCT, and then show how selection of subgroups biases estimates of the intervention effect.  


<!---Online DeclareDesign book
https://book.declaredesign.org/declaration-diagnosis-redesign/declaration-in-code.html

see also
https://macartan.github.io/ci/syllabus.pdf-->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=2)
library(knitr)
library(kableExtra)
library(gridExtra)
library(DeclareDesign)
library(tidyverse)
library(fabricatr)
library(randomizr)
library(estimatr)
library(nlme)
library(broom)
library(broom.mixed)
require(rstatix) #for convert_as_factor
require(reshape2)
require(ggpubr)
library(rdss)  #not available for this version of R but from github
#source("helpers.R") #I found I needed this and it is just loaded into the project
#remotes::install_github("DeclareDesign/rdss")
ntab <- 0 #counter for labelling tables
set.seed(1)
```





## Key details of the original RCT

These details are reported in the original report by Worth et al. (2018). GraphoGame Rime intervention was compared with business-as-usual in a sample of 398 Year 2 pupils from 15 primary schools. This was a two-armed pupil-randomized controlled trial powered to detect a minimal detectable effect size of .17. All children were selected as having low literacy skills, as assessed by the phonics screening check, a national assessment that is taken at the end of Year 1. Final data were available for 362 children from two cohorts, each doing the intervention for one spring term in successive years. Allocation to intervention or control group was done by stratified randomization of pupils by class, to ensure roughly equal numbers of children in intervention and control groups in each class. Attrition was around 10 per cent for both intervention and control groups, and did not appear biased but rather due to chance events such as absence on the day of the test.  


GraphoGame Rime is a computerized intervention, where children are motivated to play a game that teaches understanding of relationships between letters and sounds in words. The game is adaptive, with the child progressing through different levels depending on their performance. Although the children should be supervised to ensure they can log on and remain on task, there is no active tuition by teachers. Game usage was remotely monitored, and the average playing time was six yours in the first cohort, and nine hours in the second cohort: the developer recommends that pupils should spend between 8.3 and 12.5 hours playing the game. The children in the intervention group played the game for 10-15 minutes each day during a literacy session, while those in the control group did other literacy activities.

The primary outcome was the raw score on the New Group Reading Test, developed by GL Assessment, administered by testers from NFER within a month of the intervention ending. This same measure had been administered prior to intervention (pre-test). A spelling test was also administered at post-test. 

The analysis used a single-level regression model, with class dummy coded. Raw score at post-test was the dependent variable, with intervention status (0 or 1), raw score on the pre-test, and class (fixed effect) as predictors. The standardized effect size was the coefficient on the intervention group indicator, divided by the total sample standard deviation. Hedges adjustment was applied for small sample bias of estimated variance.  

A planned subgroup analysis was conducted for the subsample of children who had free school meals (FSM) - an indicator of Pupil Premium Status. Further analyses were conducted to consider how number of hours using GraphoGame related to outcome. The reported noted, however that "Whilst this analysis appears attractive, it is very vulnerable to bias as those individuals who used the program the most are likely to have other characteristics that are associated with improved test performance" - a point that is relevant to the subsequent analysis by Ahmed et al (2020).

The evaluators noted that the correlation between pre-test and post-test scores on the reading test was only .57, lower than the anticipated value of .80. The primary analysis gave an estimated effect size of -.06 (97% CI -.23 to .12), i.e., the mean raw outcome score was marginally lower for the intervention group, but the difference was not reliably different from zero. Similar results were obtained with the spelling test. The analysis with the subgroup of FSM children did not alter the findings. The interaction between pre-test score and intervention group was not statistically significant, indicating that the impact of intervention did not vary according to initial level of attainment. There was also no clear evidence that the intervention effect differed across classrooms, although the power to detect such effects was very low.  Finally, there was a negative correlation between the amount of time spent playing GraphoGame and reading post-test scores (r = -.298): i.e. the more time the child spent playing the game, the less progress they made. 

<!---p 24 We found a negative correlation between the amount of time pupils spent using GraphoGame and reading post-test scores: this suggests that pupils who spent longer playing the game were pupils who made less progress in reading. This difference also remains after taking account of pupilsâ€™ pre-test scores. This does not necessarily imply that spending more time on the game caused less progress as measured by the test scores. The amount of time spent using the game was a choice made by pupils and/or teaching staff, so was not randomly assigned and could reflect other underlying differences between the pupils/ teachers.-->


## Reanalysis by Ahmed et al (2020)  
Ahmed et al reanalysed the data after dropping cases from the intervention group who had not progressed beyond the mean play progress point for the whole intervention cohort. The remaining 95 intervention cases were then compared with the whole control group on a range of outcome measures. The rationale for this approach is that playing time was very variable, and some children may have used their time alone on the computer to do other activities. Therefore it would be more realistic to restrict consideration to children who had progressed far enough through the game to indicate that they had "received sufficient independent and solitary exposure to the game to learn English phonics". 

According to this logic, one might have expected Ahmed et al (2020) to focus on a subgroup of children who had spent some minimum amount of time playing the game. However, we already know from the NFER report that more time playing the game is not associated with better progress - if anything, the converse. Instead, the authors took the subset of children who made *most progress* through the game. The game is adaptive, progressing through 25 streams of phonic knowledge. The mean point reached by all the intervention group was level 5 of Stream 16. So the authors selected a "top half" group of children who played the game beyond that point. We will refer to this as the Top-Half GG group. Repeated measures ANOVAs were run to compare progress in this group with that of the control group on the original outcome measures (NGRT) as well as two subtests from the Test of Word Reading Efficiency (TOWRE; Torgesen et al, 1999). The critical term was the interaction between Group and Test Occasion. This showed a statistically significant effect for the TOWRE Phonetic Decoding Efficiency subtest, but not for the NGRT or the TOWRE Sight Word Reading. However, these gains were not maintained when reading was reassessed after the summer holidays.

Further analyses were conducted with subgroups of the Top-Half GG group versus subsets of controls, but we will not consider these further, as they will all be affected by the bias that arises from the initial subgrouping the sample on a post-intervention variable. 

## Using DeclareDesign to compare analytic approaches

DeclareDesign is a suite of functions that allow one to simulate datasets from specified experimental designs and compare the impact of different sampling frames and analytic approaches. DeclareDesign adopts a formal approach to research design, distinguishing between four steps: Model, Inquiry, Data Strategy and Answer strategy (MIDA), which are combined to characterize a design, which can then be evaluated using simulated data. The package includes a Design Library, with R code for simulating common experimental designs (see https://declaredesign.org/r/designlibrary/). We take as our starting point the Pretest Posttest Design (https://declaredesign.org/r/designlibrary/articles/pretest_posttest.html). Although this can be run as a single step with specified parameters, we will walk through each step of code here to illustrate the MIDA approach. A potentially confusing feature of DeclareDesign is that the various 'declare' functions do not compute results, but rather specify new functions. These functions are then combined to create a full design specification.  Here we have added some additional steps to the code to make it possible to inspect what is achieved in different code chunks, and some variable names have been changed to make them more aligned with terminology used in psychology.  

<!--code for DeclareDesign is here: https://dataverse.harvard.edu/file.xhtml?fileId=7017490&version=5.2-->

### Model  

The model specifies the nature of the sample and observed variables, plus the estimated impact of an experimental manipulation (in this case intervention). For simplicity, we simulate scores as random normal deviates, with SD of 1.  We need to specify the sample size (prior to attrition), N, the correlation between pretest and posttest scores, rho, and the average treatment effect (EffSize). (In DeclareDesign examples, this is referred to as _ate_). In addition, since Worth et al reported general improvement in scores on the reading test from time 1 to time 2 of around 1 SD, regardless of intervention, this is added to time 2 scores.  This code achieves these steps, and we can see the first eight rows of the simulated data table in Table 1. 

```{r specifymodel}
N         <- 398 #full sample size, prior to attrition
sd_1      <- 1 #we work with random normal deviates, so SD = 1
gain_t1t2 <- 1 #scores improve by 1 SD on average  
sd_2      <- 1 #we work with random normal deviates, so SD = 1
rho       <- .6 # correlation between time 1 and time 2 scores, from Table 3 of NFER report is .57
EffSize   <- 0 # average treatment effect: from Table 6 of NFER report is -0.06
nsims     <- 50 #number of simulations: set to a high number for final run

population <- declare_population(
  N = N, 
  u_t1 = rnorm(N) * sd_1, 
  u_t2 = gain_t1t2+rnorm(N, rho * scale(u_t1), sqrt(1 - rho^2)) * sd_2, 
  Y_t1 = u_t1)

mypop <- population() #this step is added just to make output of function visible for explanatory purposes
potential_outcomes <- declare_potential_outcomes(Y_t2 ~ u_t2 + 
    EffSize * Z) #defaults to 2 conditions of variable Z, with values 0 or 1
mypot <- potential_outcomes(mypop) #this step is added just to make output of function visible for explanatory purposes

#Show output in a table
options(kableExtra.html.bsTable = T)
ntab<-ntab+1 #increment table counter
tabname<-paste0("Table ",ntab,": Simulated data")
tabdf <- mypot[1:8,]

knitr::kable(tabdf,escape = F, align = "c", booktabs = T,caption=tabname) %>%
  kable_styling(c("striped", "bordered"), latex_options = "striped", full_width = F)



```

The first function, _population_, generates the first four columns for 398 rows. The first column, ID, just identifies each simulated participant by a sequential number. Columns with the prefix u_ correspond to unobserved latent variables, with u_t1 representing time 1 scores and u_t2 representing time 2 scores. The code specifies that these are drawn from a population where u_1 and u_2 are correlated with correlation _rho_. For the whole dataframe, the mean of u_2 is greater than u_t1 by the value _gain_t1t2_. Columns with the prefix Y_ correspond to expected values for observed variables. Y_t1 is the same as u_1 and corresponds to the observed pretest value. Although this is redundant, it clarifies the distinction between unobserved, latent variables and observed variables. For Y_t2 there are two values generated, Y_t2_Z_0 and Y_t2_Z_1. These are potential outcomes, depending on whether the case is allocated to the control group (Z_0) or the intervention group (Z_1).  The values of Y_t2_Z_0 are the same as values of u_t2, whereas the values of Y_t2_Z_1 correspond to u_t2 plus the value of EffSize; thus the averaged treatment effect is added. 

Although this may seem a cumbersome and redundant way of simulating what are essentially a pretest and posttest score with a given level of correlation, it provides both conceptual clarity and analytic flexibility, as will become apparent at subsequent steps.

### Inquiry  
The inquiry specifies what parameter we want to estimate from the model - known as the estimand. This could be a descriptive statistic, such as the mean value of a variable, or the difference or correlation between variables. Here we specify the mean difference between intervention and control groups at time 2 as the estimand.

```{r specifyinquiry}
estimand <- declare_inquiry(EffSize = mean(Y_t2_Z_1 - Y_t2_Z_0))
mean_t1 <- declare_inquiry(mean_t1 = mean(Y_t1)) #added to demonstrate use of inquiry for descriptive data

cor12 <- declare_inquiry(cor_t1t2=cor(u_t1,u_t2)) #added to demonstrate use of inquiry for descriptive data
#estimand(mypot) #for explanatory purposes


```

If we apply the estimand function to the data frame generated at the Model stage, we get a value of the estimand of `r estimand(mypot)$estimand`.  While this is reassuring, it is hardly surprising, since we defined Y_t2_Z_0 and Y_t2_Z_1 as equivalent but with the specified _EffSize_ added to the Y_t2_Z_1 condition. However, in subsequent steps we can consider how the estimand compares with estimates of its value in subsets of simulated data, with differences between the estimand and estimates indicating how much bias there is in the analysis.  In addition, we show in the script how we can use inquiries for descriptive statistics. These are not part of the main pretest_posttest design function, but may be useful when checking the simulation against real data. The value of mean_t1 and cor_t1t2 are likely to differ from expected values of 0 and rho respectively, because they are obtained by sampling cases from a population, and so will vary with each run of the simulation.  In this run of the simulation, mean_t1 is `r mean_t1(mypot)$estimand` and cor12 is `r cor12(mypot)$estimand`.

### Data strategy

The data strategy step selects data for allocation to treatments and for analysis. In the chunk below, we first use declare_assignment to randomly assign cases to 0 (control) or 1 (intervention), and then use it again to specify whether or not the case HasData (using the attrition rate to randomly assign a proportion of cases as HasData = 0). We also make a new column that corresponds to the outcome corresponding to the intervention assignment for each case, and finally, we compute the observed difference (diff_t2t2) between posttest and pretest scores for each row.

```{r datastrategy}

assignment <- declare_assignment(Z = complete_ra(N))
mypota<-assignment(mypot) #added just for didactic purposes

attrition_rate<-.09 # from NFER report p 18
report <- declare_assignment(HasData = complete_ra(N, prob =  1-
                                               attrition_rate))
mypotb<-report(mypota) #added just for didactic purposes
reveal_t2 <- declare_reveal(Y_t2)
mypotc<-reveal_t2(mypotb)
manipulation <- declare_step(diff_t1t2 = (Y_t2 - Y_t1), 
                             handler = fabricate)
mypotd<-manipulation(mypotc)
ntab<-ntab+1
tabname<-paste0("Table ",ntab,": Simulated data after coding intervention and attrition")
tabdf <- mypotd[1:8,]

knitr::kable(tabdf,escape = F, align = "c", booktabs = T,caption=tabname) %>%
  kable_styling(c("striped", "bordered"), latex_options = "striped", full_width = F)
```

We can see that our simulated data frame now has additional columns. Column Z indicates whether the individual is assigned to control (0) or intervention (1) condition. HasData is 1 for most cases, and 0 for around 9%.  The Y_t2 column is created by selecting Y_t2_Z_0 where Z is 0, and Y_t2_Z_1 where Z is 1. Although potentially we could remove cases where HasData = 0 by setting Y_t2 to NA, this effect is achieved later on, at the Answer step.  The final column is the observed difference between scores at t2 and t1. (The latter is not used for our current analysis).

### Answer strategy

The answer strategy involves first fitting a statistical model to data, and then summarising the model fit. The example of the pretest_posttest function in the DeclareDesign vignette compares three different analytic approaches, all implemented in linear models. This is less relevant to our purposes, and so we will focus just on an analysis corresponding to that adopted in the NFER report;  predicting outcome from two variables: treatment group (Z) and pretest score (Y_t1). (In fact, the NFER report added classroom as a further covariate, but for simplicity we will ignore that here).

```{r answerstrategy}

lm_pretest <- declare_estimator(Y_t2 ~ Z + Y_t1, .method = lm_robust, 
                                 inquiry = estimand, subset = HasData == 1, label = "Pretest as covariate")

#Note that we use the subset function to restrict analysis to those where HasData = 1


# The output of this is same as for Z term with this syntax:
#  mylm<-lm_robust(Y_t2 ~ Z + Y_t1,mypotd[mypotd$HasData==1,])
ntab<-ntab+1

tablm <- lm_pretest(mypotd)
tabname<-paste0("Table ",ntab,": Output from linear model applied to simulated data")


knitr::kable(tablm,escape = F, align = "c", booktabs = T,caption=tabname) %>%
  kable_styling(c("striped", "bordered"), latex_options = "striped", full_width = F)
```

Table `r ntab` shows the effect of running the lm_pretest function on a single simulated dataset. To get a reliable estimate of the properties of this design, and the variability around the estimated parameters, we need to run the simulation multiple times.  We first create our full design by bolting together the elements of the Model, Inquiry, Data Strategy and Answer Strategy, and we can run _diagnose_design_ with a specified number of simulations.  We can also use the _redesign_ function to easily change the values of parameters and rerun the simulation to see the effect. This is pertinent to the NFER analysis. As noted by Worth et al. (2018), the study power was calculated a priori to detect an effect size of .17, assuming that the correlation between time 1 and time 2 reading data would be .8, when in fact it was just below .6. If we rerun the simulation with EffSize of .17 and rho of .8, this confirms that the power is .79, but with the actual value of .6, power is reduced to .55. With hindsight, the reading measure used at pretest was not optimal for detecting change in this kind of study because of its low reliability.

```{r declaredesign}

   
NFER_design <- population + potential_outcomes + 
  estimand + assignment + reveal_t2 + report + 
  lm_pretest

# NFER_diagnosis <- diagnose_design(NFER_design,sims=500)
# NFER_diagnosis
#Original power computation used EffSize of .17
designs <- redesign(NFER_design,EffSize=c(0,.2),rho=c(.6,.8))
NFER_diagnosis<-diagnose_designs(designs,sims=nsims)
NFER_sims <- NFER_diagnosis$simulations_df #saves a dataframe of all the simulation runs

tabNFER <- NFER_diagnosis$diagnosands_df
tabx <- t(tabNFER)
ntab<-ntab+1
tabname<-paste0("Table ",ntab,": Diagnostic results from model with different parameter settings")


knitr::kable(tabx,escape = F, align = "c", booktabs = T,caption=tabname) %>%
  kable_styling(c("striped", "bordered"), latex_options = "striped", full_width = F)

```


### Modifying the simulation to match the Ahmed et al analysis

It is relatively straightforward to extend the design to represent the selection of of Top-Half GG cases.
To do this we create a new latent variable, L, which represents the level reached by players of GraphoGame. This is modeled as a random normal deviate, as for the other latent variables. A critical issue is how far it correlates with u_t2. It seems feasible that the children who progress furthest through the game are those that learn fastest. Ahmed et al (2020) assume that this happens because practice on the game causes better reading ability, but this direction of causality cannot be assumed: there are many reasons why some children learn faster than others which could influence both the outcome measure and the rate of progress through the game. With randomized assignment, we can get a handle on causality, but once we break the randomization, this is no longer the case. In effect, the selection method adopted by Ahmed et al focuses on children in the intervention group who did well, using a proxy measure (level of game attained) which is not random. 

We can model this scenario with a new variable, r_L.u2, which represents the correlation between L and u_t2. Then we simply need to specify that for those where Z = 1 (i.e. the intervention group), we drop any cases where L is less than zero (i.e. below average). The next chunk of code performs these steps. We start by assuming r_L.u2 = .4, and modify the _population_ statement to add the new latent term, L.


```{r newparams}

r_L.u2<- .4 #correlation between R and u_t2
EffSize <- 0

population_sel <- declare_population(N = N, 
        u_t1 = rnorm(N) * sd_1, 
        u_t2 = gain_t1t2+rnorm(N, rho * scale(u_t1), sqrt(1 - rho^2)) * sd_2, 
        Y_t1 = u_t1, 
        L=rnorm(N, r_L.u2 * scale(u_t2), sqrt(1 - r_L.u2^2)) *  sd_2)

pops<-population_sel()
pops1<-potential_outcomes(pops)
assignment <- declare_assignment(Z = complete_ra(N))
pops2<-assignment(pops1) #added just for didactic purposes

attrition_rate<-.09 # from NFER report p 18
report <- declare_assignment(HasData = complete_ra(N, prob =  1-
                                               attrition_rate))
pops3<-report(pops2) #added just for didactic purposes
reveal_t2 <- declare_reveal(Y_t2)
pops4<-reveal_t2(pops3)
report2<-declare_assignment(Exclude = (L<0)&(Z==1))   #TRUE (ie exclude) if treated with below avg on L
pops5<-report2(pops4)

myancova_sel <- declare_estimator(Y_t2 ~ Z + Y_t1, .method = lm_robust, 
    inquiry = estimand, subset = (HasData==1 & Exclude==FALSE), label = "Excluding low level group 1")




NFER_design <- population + potential_outcomes + 
  estimand + assignment + reveal_t2 + report + 
  lm_pretest   #original NFER model as before

Ahmed_design <- population_sel + potential_outcomes + 
  estimand + assignment + reveal_t2 + report + report2+
  myancova_sel #contrasting Ahmed model includes report2 to retain only Top-Half GG


designs <- redesign(Ahmed_design,r_L.u2=c(0,.2,.4),EffSize=c(0,.2))
Ahmed_diagnosis<-diagnose_designs(designs,sims=nsims)
Ahmed_sims <- Ahmed_diagnosis$simulations_df #retain the simulations

tabAhmed <- Ahmed_diagnosis$diagnosands_df
tabxx <- t(tabAhmed)
ntab<-ntab+1
tabname<-paste0("Table ",ntab,": Diagnostic results from Ahmed model at two effect sizes, with different correlations between L and u_t2")


knitr::kable(tabxx,escape = F, align = "c", booktabs = T,caption=tabname) %>%
  kable_styling(c("striped", "bordered"), latex_options = "striped", full_width = F)



```


```{r savesims}



Ahmed_sims$sig <-0
Ahmed_sims$sig[Ahmed_sims$p.value<.05]<-1

NFER_sims$sig <-0
NFER_sims$sig[NFER_sims$p.value<.05]<-1


save(Ahmed_diagnosis,file=paste0("Ahmed",nsims,".RData"))
save(NFER_diagnosis,file=paste0("NFER",nsims,".RData"))

```



## Alternative analysis with 2-way mixed anova 

Rather than the regression analysis with pre-test covariate used in the NFER analysis, Ahmed et al used two-way mixed analysis of variance, with group as a between subjects variable, and time as a repeated measure. We can specify this analysis with DeclareDesign and consider if it affects the bias or sensitivity of the model.  For this analysis, the data must first be reshaped to long form, with a new column, time. We write a function, reshaper, to achieve this.

```{r withAnova}

r_L.u2<- .4 #correlation between R and u_t2
EffSize <- 0

population_sel <- declare_population(N = N, 
        u_t1 = rnorm(N) * sd_1, 
        u_t2 = gain_t1t2+rnorm(N, rho * scale(u_t1), sqrt(1 - rho^2)) * sd_2, 
        Y_t1 = u_t1, 
        L=rnorm(N, r_L.u2 * scale(u_t2), sqrt(1 - r_L.u2^2)) *  sd_2)

pops<-population_sel()
pops1<-potential_outcomes(pops)
estimand <- declare_inquiry(EffSize = mean(Y_t2_Z_1 - Y_t2_Z_0))
assignment <- declare_assignment(Z = complete_ra(N))
pops2<-assignment(pops1) #added just for didactic purposes

attrition_rate<-.09 # from NFER report p 18
report <- declare_assignment(HasData = complete_ra(N, prob =  1-
                                               attrition_rate))
pops3<-report(pops2) #added just for didactic purposes
reveal_t2 <- declare_reveal(Y_t2)
pops4<-reveal_t2(pops3)
report2<-declare_assignment(Exclude = (L<0)&(Z==1))   #TRUE (ie exclude) if treated with below avg on L
pops5<-report2(pops4)


#convert to long form

reshaper <- 
  function(data){
    data |> 
      gather(key = "time", value = "score", Y_t1, Y_t2) |> 
      convert_as_factor(ID, time)
  }


myreshape <- declare_assignment(handler=reshaper)
mypote<-myreshape(pops5)


# now run lme
anova2way <- declare_estimator(score ~ Z * time, random=~time|ID, 
                               .method = lme, 
                               .summary=tidy,
                                term = "Z:timeY_t2",
                                inquiry = "EffSize",
                                subset = (HasData == 1 & Exclude==FALSE), 
                                label = "Interaction: mixed anova")

Ahmed_anova <- population_sel + potential_outcomes + 
  estimand+ assignment + reveal_t2 + report + report2+ myreshape + 
  anova2way #

designs <- redesign(Ahmed_anova,r_L.u2=c(0,.2,.4),EffSize=c(0,.2))
Anova_diagnosis<-diagnose_designs(designs,sims=nsims)

save(Anova_diagnosis,file=paste0("Anova",nsims,".RData"))

Ahmed_anova_sims<-Anova_diagnosis$simulations_df
Ahmed_anova_sims$sig<-0
Ahmed_anova_sims$sig[Ahmed_anova_sims$p.value<.05]<-1

ggplot(Ahmed_anova_sims,aes(estimate)) + 
    geom_histogram(data=subset(Ahmed_anova_sims,sig == 1),fill = "red", alpha = 0.4) +
    geom_histogram(data=subset(Ahmed_anova_sims,sig == 0),fill = "blue", alpha = 0.4) +
    geom_vline(aes(xintercept = estimand),
             lty = "dashed", color = "#C6227F") +
    facet_grid(r_L.u2~EffSize) +
  labs(x = "Estimate", y = "Count of simulations") +
  ggtitle("Ahmed anova method: rho = .6")


```

```{r plotsims}
# plot simulations

plotsims<-function(myfile,myvar1,myvar2,longvar1,longvar2,mytitle){
c1<-which(names(myfile)==myvar1)
c2<-which(names(myfile)==myvar2)
myfile$myvar1<-paste0(longvar1,myfile[,c1])
myfile$myvar2<-paste0(longvar2,myfile[,c2])
mylabs<-c(myvar1,myvar2)
ggplot(myfile,aes(estimate)) + 
    geom_histogram(data=subset(myfile,sig == 1),fill = "red", alpha = 0.4) +
    geom_histogram(data=subset(myfile,sig == 0),fill = "blue", alpha = 0.4) +
    geom_vline(aes(xintercept = estimand),
             lty = "dashed", color = "#C6227F") +
    facet_grid(myvar1~myvar2) +
  labs(x = "Estimate", y = "Count of simulations") +
  ggtitle(mytitle)
}
```

```{r dofacetplots}

myfile<-NFER_sims
myvar1<-'rho'
myvar2<-'EffSize'
longvar1<-"Pre/post rho: "
longvar2<-"Intervention effect size: "
mytitle<-"NFER method: rho = .6 or .8"
plotNFER<-plotsims(myfile,myvar1,myvar2,longvar1,longvar2,mytitle)

myfile<-Ahmed_sims
myvar1<-"r_L.u2"
myvar2<-"EffSize"
longvar1<-"L/outcome r: "
longvar2<-"Intervention effect size: "

mytitle <- "Ahmed method: rho = .6"
plotAhmed<-plotsims(myfile,myvar1,myvar2,longvar1,longvar2,mytitle)

myfile<-Ahmed_anova_sims
myvar1<-"r_L.u2"
myvar2<-"EffSize"
longvar1<-"L/outcome r: "
longvar2<-"Intervention effect size: "
mytitle<- "Ahmed anova method: rho = .6"
plot_Ahmedanova<-plotsims(myfile,myvar1,myvar2,longvar1,longvar2,mytitle)

#For direct comparison of Ahmed regression and anova can combine in one file and facet by method.

mycols<-intersect(names(Ahmed_sims),names(Ahmed_anova_sims))
allAhmed<-rbind(Ahmed_sims[,mycols],Ahmed_anova_sims[,mycols])
allAhmed$method='2 x 2 Anova'
allAhmed$method[allAhmed$estimator=='Excluding low level group 1']<-'Covariate'

myfile<-allAhmed[allAhmed$estimand==0,]
myvar1<-"r_L.u2"
myvar2<-"method"
longvar1<-"L/outcome r: "
longvar2<-"Analysis: "
mytitle<- "Ahmed analysis: pre/posttest rho = .6"
plot_AhmedAll<-plotsims(myfile,myvar1,myvar2,longvar1,longvar2,mytitle)
ggsave('AhmedAll.png')
```

