---
title: "When alternative analyses of the same data come to different conclusions: using DeclareDesign with a worked real-world example"
author: "Dorothy Bishop & Charles Hulme"
date: "26 Feb 2024"
editor: visual
format:
  docx:
    toc: false
    number-sections: false
---

```{=html}
<!---Online DeclareDesign book
https://book.declaredesign.org/declaration-diagnosis-redesign/declaration-in-code.html

see also
https://macartan.github.io/ci/syllabus.pdf-->
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=2)
library(knitr)
library(kableExtra)
library(gridExtra)
library(DeclareDesign)
library(tidyverse)
library(fabricatr)
library(randomizr)
library(estimatr)
library(nlme)
library(broom)
library(broom.mixed)
require(rstatix) #for convert_as_factor
require(reshape2)
require(ggpubr)
library(fishmethods) #for intracluster correlation
library(rdss)  #not available for this version of R but from github
#source("helpers.R") #I found I needed this and it is just loaded into the project
#remotes::install_github("DeclareDesign/rdss")
ntab <- 0 #counter for labelling tables
nfig<-0
set.seed(1)

htmlout<-0 #1 for html. Have to avoid kable tables if you want Word (set myoutput to 2)
```

# Abstract

A large-scale randomised controlled trial (RCT) was conducted to evaluate effectiveness of GraphoGame, a popular intervention to enhance children's reading skills. In 2018 the evaluation report of the RCT concluded the intervention was ineffective, yet two years later, a re-analysis claimed that the intervention showed significant benefits. There were five key differences between the original analysis and re-analysis: (1) reliability of the main outcome measure; (2) whether the analysis took into account clustering by classroom; (3) whether regression or repeated measures Analysis of Variance was used; (4) inclusion of multiple outcomes vs a single main outcome; and (5) selection of participants. We demonstrate use of a simulation package, DeclareDesign, to evaluate the impact of these different analytic choices. The simulations showed that (i) the low reliability of the pre-registered outcome measure in the initial analysis affected statistical power and could potentially have led to a false negative. (ii) In practice, clustering effects by classroom were small and their inclusion had little impact on results; (iii) The use of ANOVA rather than regression had only a minor impact; (iv) Failure to correct for multiple comparisons inflates the false positive rate. (v) Finally, removal of a subset of the intervention group in the re-analysis can introduce substantial bias in the estimated intervention effect. We recommend use of DeclareDesign at the stage of planning a study to quantify the sensitivity and bias in different designs and optimise selection of measures and methods.

# Alternative analyses: different conclusions

There are several studies in the social sciences literature that show that different researchers analysing the same dataset can come to different conclusions. Among the reasons for discrepancies are differences in specification of the research question (Kummerfeld & Jones, 2023), but even when a question is clearly defined, differences can emerge because of analytic decisions (e.g., Silberzahn et al., 2018). Bishop and Thompson (2024) noted that in trials of interventions, we need to on the one hand minimize the impact of random noise on our estimates, and on the other hand be alert to the possibility of methodological choices that can introduce systematic bias. Most researchers will attempt to address these issues by following well-established methods in their field of study, but it is rare to see any systematic attempt to compare analytic approaches at the point of planning a study.

DeclareDesign is a suite of functions that allow one to simulate datasets from specified experimental designs and compare the impact of different sampling frames and analytic approaches (Blair, Coppock & Humphreys, 2023; https://declaredesign.org/). DeclareDesign was developed in the field of political science but is not widely used in other fields, despite its considerable potential for researchers and policymakers. The benefit of using simulations is that one can create datasets where the true values of estimates are known, and then see how well those values are recovered in an analysis. DeclareDesign adopts a formal approach to research design, distinguishing between four steps: Model, Inquiry, Data Strategy and Answer strategy (MIDA), which are combined to characterise a design, which can then be evaluated using simulated data. A useful feature of the package is that once a research design has been formally specified, it is easy to manipulate parameters to see the impact on properties of the design such as statistical power and bias in estimates.

In this article we illustrate how DeclareDesign can be used to compare different approaches to analysing data from an intervention trial. We take a specific example of recent work evaluating results from a large randomised controlled trial (RCT) on the effectiveness of GraphoGame (https://graphogame.com), a computer-based game designed to improve children's reading. We show how to simulate alternative approaches to analysis, to clarify which aspects of design can affect sensitivity and bias of estimates of intervention effects. The simulations help reconcile apparently contradictory conclusions about the effectiveness of the intervention.

# Background

GraphoGame is a set of adaptive online computer games designed to teach pupils to read (decode print) by developing their phonic reading skills (the ability to map letters in printed words onto words pronunciations). The games were initially developed at the University of Jyväskylä in Finland as a way of teaching children with dyslexia. Since then, GraphoGame it has been adapted and translated into versions used in over 20 countries and a company markets the programme worldwide (https://graphogame.com/). The adaptions to GraphoGame made in different language limit exact comparisons across studies because it is a learning platform and an instructional approach, rather than a single game with fixed properties. The predominant English version of GraphoGame, is referred to GraphoGame Rime and was developed to implement the Rhyme/Analogy theory of reading development proposed Goswami and her colleagues (Goswami & East, 2000). This approach to reading is somewhat controversial and not consistent with the method mandated for use in primary schools in England which must use a Systematic Synthetic Phonics Programme, in which children, from the beginning of learning to read, are taught to link individual letters in printed words to their corresponding phonemes (CAT -\> /c/ /a/ /t/). In GraphoGame Rime in contrast the initial emphasis is on teaching mappings at the level of rime units (the vowel and following consonant (if any) in a syllable (CAT -\> /c/ /at/)).

## Evidence for the effectiveness of GraphoGame.

According to the company's website GraphoGame has been used by over 4 million children, and is an "effective, proven and affordable way to teach reading..." and "The most researched literacy game in the world. Developed together with Cambridge University." One critical claim about GraphoGame is that it allows children to "Achieve basic literacy in 1 school year even without parental support" and a headline on the company website even states "Teach your child to read in just days".

GraphoGame is unusual in that a number of randomized studies have been conducted to evaluate its effectiveness. A meta-analysis by McTigue et al. (2019) lists 15 randomised or quasi-experimental studies of GraphoGame (quasi-experiments are studies where students are not randomly assigned to conditions). McTigue et al. included a total of 19 independent comparisons in their meta-analysis. It is notable that the majority of the studies had small sample sizes and none of the studies considered individually yielded statistically significant improvements in word reading as a result of GraphoGame. The overall effect size from the metanalysis (g = -0.02) was almost exactly zero.

By far the largest randomised study of GraphoGame which was included in the meta-analysis was an independent evaluation of GraphoGame Rime conducted in England by Worth et al (2018). This was a randomised trial with children selected for poor reading skills within classrooms assigned to GraphoGame Rime or control. The effect size from this study was g = -0.06. This was a well conducted trial although it suffered from a poorly chosen outcome measure (the New Group Reading Test) which is a complex blend of decoding and comprehension measures, and the measure was at floor at pretest because it was too difficult for many of the children. However, a secondary outcome measure was a single word spelling test which arguably was a more sensitive measure what was better aligned with the GraphoGame treatment (since it is a more direct measure of phonic skills). The effects of GraphoGame were however again null on this measure (g = 0.01). The conclusion from this independently conducted evaluation is stark: "The trial found no evidence that GraphoGame Rime improves pupils' reading or spelling test scores when compared to business-as-usual. This result has very high security". The high security rating refers to Education Endowment Foundation's rating of the quality of this randomised trial. This coupled with the null results from the other studies reviewed by McTigue provide strong evidence that GraphoGame is not effective as a method of teaching reading.

Ahmed et al. (2020) conducted a reanalysis of data collected in an RCT of GraphoGame Rime (https://graphogame.com), a computerised game for training English phonics knowledge in poor readers. The original research was funded by the Education Endowment Foundation and Wellcome Trust, and involved 398 Year 2 pupils in 15 primary schools. In sharp contrast to the original report on outcomes from the RCT, Ahmed et al. (2020) concluded that "The current study suggests that young learners of the English orthography show significant benefits in learning both phonic decoding skills and spelling skills from the supplementary use of GG Rime in addition to ongoing classroom literacy instruction." Those considering using Graphogame will find it confusing that such diametrically opposed conclusions can be drawn from analysis of the same dataset, and here we show how modeling the contrasting analytic approaches can clarify the sources of disagreement.

<!---see also registration on http://www.isrctn.com/ISRCTN10467450-->

# Details of the original RCT

These details are reported in the original report by Worth et al. (2018). All participants were selected as having low literacy skills, as assessed by the phonics screening check, a national assessment that is taken at the end of Year 1.G raphoGame Rime intervention was compared with business-as-usual in a sample of 398 Year 2 pupils from 15 primary schools. GraphoGame training occurred during literacy sessions, when the control group children received regular literacy activities with the class teacher. This was a two-armed pupil-randomised controlled trial powered to detect a minimal detectable effect size of .17. Final data were available for 362 children from two cohorts, each doing the intervention for one spring term in successive years. Allocation to intervention or control group was done by stratified randomisation of pupils by classroom, to ensure roughly equal numbers of children in intervention and control groups in each classroom. Attrition was around 10 per cent for both intervention and control groups, and did not appear biased but rather due to chance events such as absence on the day of the test.

GraphoGame Rime is a computerised intervention, where children are motivated to play a game that teaches understanding of relationships between letters and sounds in words. The game is adaptive, with the child progressing through different levels depending on their performance. Although the children should be supervised to ensure they can log on and remain on task, there is no active tuition by teachers. Game usage was remotely monitored, and the average playing time was six yours in the first cohort, and nine hours in the second cohort: the developer recommends that pupils should spend between 8.3 and 12.5 hours playing the game. The children in the intervention group played the game for 10-15 minutes each day during a literacy session, while those in the control group did other literacy activities.

The primary outcome was the raw score on the New Group Reading Test, developed by GL Assessment, administered by testers from NFER within a month of the intervention ending. This same measure had been administered prior to intervention (pre-test). A spelling test was also administered at post-test.

The analysis used a single-level regression model, with classroom dummy coded. Raw score at post-test was the dependent variable, with intervention status (0 or 1), raw score on the pre-test, and classroom (fixed effect) as predictors. The standardised effect size was the coefficient on the intervention group indicator, divided by the total sample standard deviation. Hedges adjustment was applied for small sample bias of estimated variance.

A planned subgroup analysis was conducted for the subsample of children who had free school meals (FSM) - an indicator of Pupil Premium Status. Further analyses were conducted to consider how number of hours using GraphoGame related to outcome. The reported noted, however that "Whilst this analysis appears attractive, it is very vulnerable to bias as those individuals who used the program the most are likely to have other characteristics that are associated with improved test performance" - a point that is relevant to the subsequent analysis by Ahmed et al (2020).

The evaluators noted that the correlation between pre-test and post-test scores on the reading test was only .57, lower than the anticipated value of .80. The primary analysis gave an estimated effect size of -.06 (97% CI -.23 to .12), i.e., the mean raw outcome score was marginally lower for the intervention group, but the difference was not reliably different from zero. Similar results were obtained with the spelling test. The analysis with the subgroup of FSM children did not alter the findings. The interaction between pre-test score and intervention group was not statistically significant, indicating that the impact of intervention did not vary according to initial level of attainment. There was also no clear evidence that the intervention effect differed across classrooms, although the power to detect such effects was very low. Finally, there was a negative correlation between the amount of time spent playing GraphoGame and reading post-test scores (r = -.298): i.e. the more time the child spent playing the game, the less progress they made.

<!---p 24 We found a negative correlation between the amount of time pupils spent using GraphoGame and reading post-test scores: this suggests that pupils who spent longer playing the game were pupils who made less progress in reading. This difference also remains after taking account of pupils’ pre-test scores. This does not necessarily imply that spending more time on the game caused less progress as measured by the test scores. The amount of time spent using the game was a choice made by pupils and/or teaching staff, so was not randomly assigned and could reflect other underlying differences between the pupils/ teachers.-->

## Reanalysis by Ahmed et al (2020)

Ahmed et al reanalysed the data after dropping cases from the intervention group who had not progressed beyond the mean play progress point for the whole intervention cohort. The remaining 95 intervention cases were then compared with the whole control group on a range of outcome measures. The rationale for this approach is that playing time was very variable, and some children may have used their time alone on the computer to do other activities. Therefore it would be a fairer test to restrict consideration to children who had progressed far enough through the game to indicate that they had "received sufficient independent and solitary exposure to the game to learn English phonics".

According to this logic, one might have expected Ahmed et al (2020) to focus on a subgroup of children who had spent some minimum amount of time playing the game. However, we already know from the NFER report that more time playing the game is not associated with better progress - if anything, the converse. Instead, the authors took the subset of children who made *most progress* through the game. The game is adaptive, progressing through 25 streams of phonic knowledge. The mean point reached by all the intervention group was level 5 of Stream 16. So the authors selected a "top half" group of children who played the game beyond that point. We will refer to this as the Top-Half GG group. Repeated measures ANOVAs were run to compare progress in this group with that of the control group on the original outcome measures (NGRT) as well as two subtests from the Test of Word Reading Efficiency (TOWRE; Torgesen et al, 1999). The critical term was the interaction between Group and Test Occasion. This showed a significantly larger improvement in the GG group on the TOWRE Phonetic Decoding Efficiency subtest, but not for the NGRT,the TOWRE Sight Word Reading subtest, nor the Single Word Spelling test. However, the difference in gains between groups on the TOWRE Phonetic Decoding Efficiency subtest were not maintained when it was reassessed after the school summer holidays.

Further analyses were conducted with subgroups of the Top-Half GG group versus subsets of controls, but we will not consider these further, as they will all be affected by any bias that arises from the initial subgrouping the sample on a post-intervention variable.

There are at least five aspects of the analysis that differed between the NFER and Ahmed et al analyses:

1)  *Reliability of outcome measures*. The NFER analysis used a preregistered outcome measure, the New Group Reading Test (NGRT). This proved to be problematic; the original plan was to use a version of the test (Level 2) that was too difficult, so Level 1B was substituted. The reliability of this test was lower than anticipated. The Ahmed et al analysis used two subtests from the TOWRE and a spelling test.

2)  *Control for clustering by classroom*. The NFER analysis included a term in the regression analysis that took into account clustering of scores by classroom. This was not done in the Ahmed et al analysis.

3)  *Robust regression vs. repeated measures ANOVA*. The Ahmed et al analysis treated pretest and posttest readings scores as two levels of a dependent variable in repeated measures Analysis of Variance, whereas in the original analysis, linear regression was used with pretest as a covariate.

4)  *Use of multiple outcomes*. The NFER analysis focused on a single pre-registered outcome measure. The Ahmed et al analysis explored results from several related outcome measures.

5)  *Selection of participants for analysis*. The NFER analysis included all participants in the RCT for whom outcome data was available. The Ahmed et al analysis dropped half of the intervention group who were below average in the level of the game they had achieved, on the grounds that they may not have been engaged with the intervention.

There is a large literature on design of clinical trials that discusses the impact of different analytic approaches, but this is less well-known outside the medical sphere in fields such as education, and even where researchers are aware of recommendations, they may not appreciate how far departures from usual practice have serious vs trivial consequences. Here we used simulated data to illustrate the impact of each of these analytic choices, using the DeclareDesign package (Blair, Coppock, & Humphreys, 2023) in the R computing language (R Core Team, 2023).

## Model A. Using DeclareDesign to simulate original analytic model

We started by simulating data from the original trial, which adopted a fairly standard approach for analysing a Randomised Controlled Trial (Shadish, Cook, & Campbell, 2002), where children were randomly assigned to Intervention or Control groups, with regression analysis used to compare their performance on a preregistered outcome measure, with a pre-intervention score used as a covariate.

The DeclareDesign package includes a Design Library, with R code for simulating common experimental designs (see https://declaredesign.org/r/designlibrary/). We took as our starting point the Pretest Posttest Design (https://declaredesign.org/r/designlibrary/articles/pretest_posttest.html), which implements the type of analysis used by Worth et al (2018). Although this can be run as a single step with specified parameters, we will walk through each step of code here to illustrate the MIDA approach. A potentially confusing feature of DeclareDesign is that the various 'declare' functions do not compute results, but rather specify new functions. These functions are then combined to create a full design specification. Here we have added some additional steps to the code to make it possible to inspect what is achieved in different code chunks, and some variable names have been changed to make them more aligned with terminology used in psychology.

<!--code for DeclareDesign is here: https://dataverse.harvard.edu/file.xhtml?fileId=7017490&version=5.2-->

### Model

The model specifies the nature of the sample and observed variables, plus the estimated impact of an experimental manipulation (in this case intervention). For simplicity, we simulate scores as random normal deviates, with SD of 1. We need to specify the sample size (prior to attrition), N, the correlation between pretest and posttest scores, *rho*, and the intervention effect, *EffSize*. (In DeclareDesign examples, this is referred to as *ate* or *Average Treatment Effect*). In addition, since Worth et al reported general improvement in scores on the reading test from time 1 to time 2 of around 1 SD, regardless of intervention, this is added to time 2 scores. This code achieves these steps, and we can see the first eight rows of the simulated data table in Table 1.

```{r specifymodel}
N         <- 398 #full sample size, prior to attrition
sd_1      <- 1 #we work with random normal deviates, so SD = 1
gain_t1t2 <- 1 #scores improve by 1 SD on average  
sd_2      <- 1 #we work with random normal deviates, so SD = 1
rho       <- .6 # correlation between time 1 and time 2 scores, from Table 3 of NFER report is .57
EffSize   <- 0 # average treatment effect: from Table 6 of NFER report is -0.06
nsims     <- 50 #number of simulations: set to a high number for final run



population <- declare_population(
  N = N, 
  u_t1 = rnorm(N) * sd_1, 
  u_t2 = gain_t1t2+rnorm(N, rho * scale(u_t1), sqrt(1 - rho^2)) * sd_2, 
  Y_t1 = u_t1)

mypop <- population() #this step is added just to make output of function visible for explanatory purposes
potential_outcomes <- declare_potential_outcomes(Y_t2 ~ u_t2 + 
    EffSize * Z) #defaults to 2 conditions of variable Z, with values 0 or 1
mypot <- potential_outcomes(mypop) #this step is added just to make output of function visible for explanatory purposes

#Show output in a table
options(kableExtra.html.bsTable = T)
ntab<-ntab+1 #increment table counter
tabname<-paste0("Table ",ntab,": Simulated data")
tabdf <- mypot[1:8,]

if(htmlout<-1){
 knitr::kable(tabdf,escape = F, align = "c", booktabs = T,caption=tabname) %>%
  kable_styling(c("striped", "bordered"), latex_options = "striped", full_width = F)
}
if(htmlout<-0)
  print(tabdf)



```

The first function, *population*, generates the first four columns for 398 rows. The first column, ID, just identifies each simulated participant by a sequential number. Columns with the prefix u\_ correspond to unobserved latent variables, with u_t1 representing time 1 scores and u_t2 representing time 2 scores. The code specifies that these are drawn from a population where u_1 and u_2 are correlated with correlation *rho*. For the whole dataframe, the mean of u_2 is greater than u_t1 by the value *gain_t1t2*. Columns with the prefix Y\_ correspond to expected values for observed variables. Y_t1 is the same as u_1 and corresponds to the observed pretest value. Although this is redundant, it clarifies the distinction between unobserved, latent variables and observed variables. For Y_t2 there are two values generated, Y_t2_Z_0 and Y_t2_Z_1. These are potential outcomes, depending on whether the case is allocated to the control group (Z_0) or the intervention group (Z_1). The values of Y_t2_Z_0 are the same as values of u_t2, whereas the values of Y_t2_Z_1 correspond to u_t2 plus the value of EffSize; thus the averaged treatment effect is added.

Although this may seem a redundant way of simulating what are essentially a pretest and posttest score with a given level of correlation, it provides both conceptual clarity and analytic flexibility, as will become apparent at subsequent steps.

### Inquiry

The inquiry specifies what parameter we want to estimate from the model - known as the estimand. This could be a descriptive statistic, such as the mean value of a variable, or the difference or correlation between variables. Here we specify the mean difference between intervention and control groups at time 2 as the estimand.

```{r specifyinquiry}
estimand <- declare_inquiry(EffSize = mean(Y_t2_Z_1 - Y_t2_Z_0))
mean_t1 <- declare_inquiry(mean_t1 = mean(Y_t1)) #added to demonstrate use of declare_inquiry for descriptive data

cor12 <- declare_inquiry(cor_t1t2=cor(u_t1,u_t2)) #added to demonstrate use of declare_inquiry for descriptive data
#estimand(mypot) # uncomment to see the estimand for the mypot dataframe


```

If we apply the estimand function to the data frame generated at the Model stage, we get a value of the estimand of `r estimand(mypot)$estimand`. While this is reassuring, it is hardly surprising, since we defined Y_t2_Z_0 and Y_t2_Z_1 as equivalent but with the specified *EffSize* added to the Y_t2_Z_1 condition. However, in subsequent steps we can consider how the estimand compares with estimates of its value in subsets of simulated data, with differences between the estimand and estimates indicating how much bias there is in the analysis. In addition, we show in the script how we can use inquiries for descriptive statistics. These are not part of the main pretest_posttest design function, but may be useful when checking the simulation against real data. The value of mean_t1 and cor_t1t2 are likely to differ from expected values of 0 and rho respectively, because they are obtained by sampling cases from a population, and so will vary with each run of the simulation. In this run of the simulation, mean_t1 is `r mean_t1(mypot)$estimand` and cor12 is `r cor12(mypot)$estimand`.

### Data strategy

The data strategy step selects data for allocation to treatments and for analysis. In the chunk below, we first use declare_assignment to randomly assign cases to 0 (control) or 1 (intervention), and then use it again to specify whether or not the case HasData (using the attrition rate to randomly assign a proportion of cases as HasData = 0). We also make a new column that corresponds to the outcome corresponding to the intervention assignment for each case, and finally, we compute the observed difference (diff_t1t2) between posttest and pretest scores for each row.

```{r datastrategy}

assignment <- declare_assignment(Z = complete_ra(N))
mypota<-assignment(mypot) #added just for didactic purposes

attrition_rate<-.09 # from NFER report p 18
report <- declare_assignment(HasData = complete_ra(N, prob =  1-
                                               attrition_rate))
mypotb<-report(mypota) #added just for didactic purposes
reveal_t2 <- declare_reveal(Y_t2)
mypotc<-reveal_t2(mypotb)
manipulation <- declare_step(diff_t1t2 = (Y_t2 - Y_t1), 
                             handler = fabricate)
mypotd<-manipulation(mypotc)
ntab<-ntab+1
tabname<-paste0("Table ",ntab,": Simulated data after coding intervention and attrition")
tabdf <- mypotd[1:8,]
if(htmlout==1){
 knitr::kable(tabdf,escape = F, align = "c", booktabs = T,caption=tabname) %>%
   kable_styling(c("striped", "bordered"), latex_options = "striped", full_width = F)
}
if(htmlout==0){
  print(tabdf)
}
```

We can see that our simulated data frame now has additional columns. Column Z indicates whether the individual is assigned to control (0) or intervention (1) condition. *HasData* is 1 for most cases, and 0 for around 9%. The *Y_t2* column is created by selecting *Y_t2_Z_0* where *Z* is 0, and *Y_t2_Z_1* where *Z* is 1. Although potentially we could remove cases where *HasData* = 0 by setting *Y_t2* to NA, this effect is achieved later on, at the Answer step. The final column is the observed difference between scores at t2 and t1. The latter is not used for our current analysis, but will feature at a later stage.

### Answer strategy

The answer strategy involves first fitting a statistical model to data, and then summarising the model fit. The example of the *pretest_posttest* function in the DeclareDesign vignette compares three different analytic approaches, all implemented in linear models. This is less relevant to our purposes, and so we will focus just on an analysis corresponding to that adopted in the NFER report; predicting outcome from two variables: treatment group (*Z*) and pretest score (*Y_t1*). The NFER report added classroom as a further covariate, but for this initial demonstration, we will ignore that.

```{r answerstrategy}

lm_pretest <- declare_estimator(Y_t2 ~ Z + Y_t1, .method = lm_robust, 
                                 inquiry = estimand, subset = HasData == 1, label = "Pretest as covariate")

#Note that we use the subset function to restrict analysis to those where HasData = 1


# The output of this is same as for Z term with this syntax:
#  mylm<-lm_robust(Y_t2 ~ Z + Y_t1,mypotd[mypotd$HasData==1,])
ntab<-ntab+1

tablm <- lm_pretest(mypotd)
tabname<-paste0("Table ",ntab,": Output from linear model applied to simulated data")

if(htmlout==1){
knitr::kable(tablm,escape = F, align = "c", booktabs = T,caption=tabname) %>%
  kable_styling(c("striped", "bordered"), latex_options = "striped", full_width = F)
}
if(htmlout==0){
  print(tablm)
}
```

Table `r ntab` shows the effect of running the *lm_pretest* function on a single simulated dataset. To get a reliable estimate of the properties of this design, and the variability around the estimated parameters, we need to run the simulation multiple times. We first create our full design by bolting together the elements of the Model, Inquiry, Data Strategy and Answer Strategy, and we can run *diagnose_design* with a specified number of simulations. We can also use the *redesign* function to easily change the values of parameters and rerun the simulation to see the effect. This is pertinent to the NFER analysis. As noted by Worth et al. (2018), the study power was calculated a priori to detect an effect size of .17, assuming that the correlation between time 1 and time 2 reading data would be .8, when in fact it was just below .6. If we rerun the simulation with EffSize of .17 and rho of .8, this confirms that the power is .79, but with the actual value of .6, power is reduced to .55. With hindsight, the reading measure used at pretest was not optimal for detecting change in this kind of study because of its low reliability.

```{r declaredesign}

   
NFER_design <- population + potential_outcomes + 
  estimand + assignment + reveal_t2 + report + 
  lm_pretest

# NFER_diagnosis <- diagnose_design(NFER_design,sims=500)
# NFER_diagnosis
#Original power computation used EffSize of .17
designs <- redesign(NFER_design,EffSize=c(0,.2),rho=c(.6,.8))
NFER_diagnosis<-diagnose_designs(designs,sims=nsims)
NFER_sims <- NFER_diagnosis$simulations_df #saves a dataframe of all the simulation runs

tabNFER <- NFER_diagnosis$diagnosands_df
tabx <- t(tabNFER)
ntab<-ntab+1
tabname<-paste0("Table ",ntab,": Diagnostic results from model with different parameter settings")

if(htmlout==1){
 knitr::kable(tabx,escape = F, align = "c", booktabs = T,caption=tabname) %>%
   kable_styling(c("striped", "bordered"), latex_options = "striped", full_width = F)
}
if(htmlout==0){
  print(tabx)
}
```

## Model B: Modifying the simulation to incorporate classroom as a cluster

Intervention studies conducted in schools can be affected by clustering, if children within a classroom are more similar than those from different classrooms. Children from 53 classrooms were included in the trial, with an average of 7.5 pupils per class eligible for the trial, and 3.8 pupils per class randomised to receive the intervention. There is much debate about the best way to take such effects into account; West et al (2018) modelled classroom as a fixed effect, so we next modified the simulation to include this variable.

The key statistic for any clustering analysis is the intra-cluster correlation (ICC) between the cluster variable and pretest. The empirical data gave an ICC of around .15 with the pretest NGRT score. We use the empirical distribution of N per classroom to first allocate a classroom to each child, and then generate *u_t1* to have given ICC for cluster effect.

```{r makeclustered}

#----------------------------------------------------------
#These values provided by Prof Goswami from empirical data
actualICC <- .15 #intracluster correlation between cluster and pretest NGRT
classfreq<-read.csv("classfreq.csv") #N children in each classroom

#classfreq<-read.csv("altclassfreq.csv") #can uncomment this to look at effect if there are just 4 clusters of roughly equal frequency
allclasses<-unique(classfreq[,1])
#----------------------------------------------------------
myprobs<-as.vector(classfreq$Freq/sum(classfreq$Freq)) #probability of allocating child to class
M <-declare_model(
  id = add_level(
    N=sum(classfreq$Freq),
    class = sample(allclasses,N, prob=myprobs,replace=TRUE)
  )
)
mypop<-M() #can look at this to see the allocation.
#Next we need to add variables as before, but u_t1 needs to be clustered with class

M1 <-
  declare_population(
                  u_t1 = draw_normal_icc(
                    mean = 0,
                    clusters = class,
                    ICC = actualICC #value from empirical data
                  ),
                  u_t2 = gain_t1t2+rnorm(N, rho * scale(u_t1), sqrt(1 - rho^2)) * sd_2, 
                  Y_t1 = u_t1)
                
nupop<-M1(mypop)

#check clusterig
clusrho <- clus.rho(popchar=nupop$u_t1, cluster = nupop$class, type = 3, est = 0, nboot = 500)

#Now we modify the regression equation to include the classroom as cluster
lm_pretest_cluster <- declare_estimator(Y_t2 ~ Z + Y_t1, .method = lm_robust, cluster=class,fixed_effects= ~class,inquiry = estimand, subset = HasData == 1, label = "Pretest as covariate, clustered")

```

We should now be able to just substitute M and M1 in the design to get clusters

```{r nuNFER}
NFER_clustered_design <- M + M1 + potential_outcomes + 
  estimand + assignment + reveal_t2 + report + 
  lm_pretest_cluster   

designs <- redesign(NFER_clustered_design,EffSize=c(0,.2),rho=c(.6,.8),actualICC=c(.15,.3,.45))
NFER_clustered_diagnosis<-diagnose_designs(designs,sims=nsims)
NFER_clustered_sim <- NFER_clustered_diagnosis$simulations_df #saves a dataframe of all the simulation runs
NFER_clustered_diags<-NFER_clustered_diagnosis$diagnosands_df


```

The analysis confirms that the clustering into classrooms has a negligible effect on the analysis, even when the ICC level is high. Clustering does reduce power when there is a much smaller number of clusters, as can be seen by changing the file that is read in to give class sizes (see supplementary figure x).

## Model C: Comparing repeated measures Anova vs linear regression

Ahmed et al (2020) treated pretest and posttest scores as different levels of a dependent variable in a mixed Analysis of Variance, where group was the between-subjects factor and time as the within-subjects factor. The intervention effect is then estimated from the interaction term. Mathematically, this analysis is equivalent to a group comparison of the time 2 vs time 1 difference scores for the two groups. We can therefore readily check the impact of this analytic decision using code that is included in the *pretest_posttest* function in the DeclareDesign vignette. (https://declaredesign.org/r/designlibrary/articles/pretest_posttest.html).

```{r changescore}
changescore <- declare_estimator(diff_t1t2 ~ Z, .method = lm_robust, 
    inquiry = estimand, subset = HasData == 1, label = "Change score")

lm_pretest <- declare_estimator(Y_t2 ~ Z + Y_t1, .method = lm_robust, 
                                 inquiry = estimand, subset = HasData == 1, label = "Pretest as covariate")

precov_vs_change <- population + potential_outcomes  +
  estimand + assignment + reveal_t2 + report  + manipulation+
  lm_pretest+changescore

diagnosis <- diagnose_design(precov_vs_change,sims=100)

designs_change <- redesign(precov_vs_change,EffSize=c(0,.2),rho=c(.6,.8))
NFER_change_diagnosis<-diagnose_designs(designs_change,sims=nsims)

```

This analysis confirms that the power is higher for the regression analysis than for the repeated measures ANOVA, but the difference is slight. It seems unlikely that the difference in conclusions between Worth et al and Ahmed et al are explicable in terms of differences in the reliability of the outcome measure, the inclusion of a term for clustering, or use of ANOVA rather than regression with pretest as a covariate.

## Model D: Including multiple outcomes

It is well-established that the false positives will increase if several outcomes are analysed without any statistical correction. Nevertheless, the standard recommendation to have a single outcome measure is not necessarily optimal in educational trials, where there may be a range of outcome measures that might be expected to be influenced by the intervention. Bishop (2023) compared different approaches to handling this situation, noting that the popular method of applying a Bonferroni correction is over-conservative when outcome measures are correlated. Taking a single principal component as the dependent variable is the best way of achieving optimal statistical power without increasing the false positive rate when a suite of outcome measures are positively correlated and may be reasonably assumed to measure a common underlying construct. An alternative approach, MEff, is similar to Bonferroni correction, but adjusts for intercorrelations between measures, and may be used when a set of outcome measures is more heterogeneous. Here we show how DeclareDesign can be used to simulate the situation when there are three correlated outcome measures.

To generate correlated measures in the simulation we used the correlation matrix for four outcome measures (the two TOWRE scales, NGRT and the Spelling test) kindly provided by Usha Goswami.

```{r trymultivar}
# Use actual correlation matrix from EEF data
mycors<-read.csv('EEF_data/cormatrix.csv')
mynames<-mycors[,1]
mycormat<-as.matrix(mycors[,2:ncol(mycors)])
M5 <-
  declare_model(
    N = 390,
    draw_multivariate(c('b_t1','u_t1a','u_t1b','u_t1c','b_t2','u_t2a','u_t2b','u_t2c','u_t2d','highstream')~ MASS::mvrnorm(
      n = 390,
      mu = c(0,0,0,0,1,1,1,1,1,0), #time 2 reading vars have effect size of 1 added
      Sigma = mycormat)
    ))

mypop <- M5() #this step is added just to make output of function visible for explanatory purposes
#NB mypop names start with X. and end with .

#potential outcomes - tried doing this in one command but it did not work, so separate command for each step in sequence
potential_outcomes_a <- declare_potential_outcomes(Y_t2a ~ X.u_t2a. + EffSize * Z)
potential_outcomes_b <- declare_potential_outcomes(Y_t2b ~ X.u_t2b. + EffSize * Z)
potential_outcomes_c <- declare_potential_outcomes(Y_t2c ~ X.u_t2c. + EffSize * Z)
potential_outcomes_d <- declare_potential_outcomes(Y_t2d ~ X.u_t2d. + EffSize * Z)
##defaults to 2 conditions of variable Z, with values 0 or 1

mypota <- potential_outcomes_a(mypop) #this step is added just to make output of function visible for explanatory purposes
mypotb<-potential_outcomes_b(mypota)
mypotc<-potential_outcomes_c(mypotb)
mypotd<-potential_outcomes_d(mypotc)

estimanda <- declare_inquiry(
  EffSize_a = mean(Y_t2a_Z_1 - Y_t2a_Z_0))
estimandb <- declare_inquiry(
  EffSize_b = mean(Y_t2b_Z_1 - Y_t2b_Z_0))
estimandc <- declare_inquiry(
  EffSize_c = mean(Y_t2c_Z_1 - Y_t2c_Z_0))

#All estimands are zero

#datastrategy}

assignment <- declare_assignment(Z = complete_ra(N))

attrition_rate<-.09 # from NFER report p 18
report <- declare_assignment(HasData = complete_ra(N, prob =  1-
                                               attrition_rate))

reveal_t2a <- declare_reveal(Y_t2a)
reveal_t2b <- declare_reveal(Y_t2b)
reveal_t2c <- declare_reveal(Y_t2c)
reveal_t2d <- declare_reveal(Y_t2d)

#Use the corresponding pretest measure for the 3 measures with pre and post- 
lm_pretest_a <- declare_estimator(Y_t2a ~ Z + X.u_t1a., .method = lm_robust, 
                                 inquiry = estimanda, subset = HasData == 1, label = "A_Pretest as cov")
lm_pretest_b <- declare_estimator(Y_t2b ~ Z + X.u_t1b., .method = lm_robust, 
                                 inquiry = estimandb, subset = HasData == 1, label = "B_Pretest as cov")
lm_pretest_c <- declare_estimator(Y_t2c ~ Z + X.u_t1c., .method = lm_robust, 
                                 inquiry = estimandc, subset = HasData == 1, label = "C_Pretest as cov")

M5_design <- M5 + potential_outcomes_a + potential_outcomes_b + potential_outcomes_c + 
  estimanda + estimandb + estimandc + assignment + reveal_t2a + reveal_t2b + reveal_t2c + report + 
  lm_pretest_a + lm_pretest_b+ lm_pretest_c  



M5_designs <- redesign(M5_design,EffSize=c(0,.2))

M5_diagnosis<-diagnose_designs(M5_designs,sims=nsims)


M5sims <- M5_diagnosis$simulations_df #retain the simulations

 M5sims$sig<-0
 M5sims$sig[M5sims$p.value<.05]<-1
 
 #We want to count the runs where ANY of outcomes a, b or c is significant
 #Each simulation is identified by sim_id, and there are 6 simulations with that ID; one for each measure and one for each effect size.
 
 M5sigs<-aggregate(M5sims$sig,by=list(M5sims$sim_ID,M5sims$EffSize),FUN=sum)
 
M5sigstab<-table(M5sigs$Group.2,M5sigs$x)
fprate <- 1-M5sigstab[1,1]/nsims
```

If we analyse three outcomes from this simulation using simple linear regression, with a pretest as covariate, then the probability of a false positive (i.e., a p-value below .05 on at least one measure) increases to `r fprate`.

## Model E: Modifying the selection of cases to match the Ahmed et al analysis

It is relatively straightforward to extend the design to represent the selection of of Top-Half GG cases. To do this we create a new latent variable, L, which represents the level reached by players of GraphoGame. This is modeled as a random normal deviate, as for the other latent variables. A critical issue is how far it correlates with u_t2 (the level of the unobserved latent reading ability at time 2). It seems feasible that the children who progress furthest through the game are those that learn fastest. Ahmed et al (2020) assume that this happens because practice on the game causes better reading ability, but this direction of causality cannot be assumed: there are many reasons why some children learn faster than others which could influence both the outcome measure and the rate of progress through the game. It could be that children who learn faster on the game are those with a higher propensity to learn to read well (for reasons not specified); in other words, selecting children based on the level attained on the GG game, may be artifactually selecting children who are going to learn to read best, whatever teaching they receive. Such an effect may be nothing to do with the time spent on GraphoGame. With randomised assignment, we can get a handle on causality, but once we break the randomisation, this is no longer the case. In effect, the selection method adopted by Ahmed et al focuses on children in the intervention group who did well, using a proxy measure (level of game attained) which is not random.

We can model this scenario with a new variable, r_L.u2, which represents the correlation between L (the Level reached on GraphoGame) and u_t2 (the unobserved latent reading ability at time 2). Then we simply need to specify that for those where Z = 1 (i.e. the intervention group), we drop any cases where L is less than zero (i.e. below average). The next chunk of code performs these steps. We start by assuming r_L.u2 = .4, and modify the *population* statement to add the new latent term, L.

```{r newparams}

r_L.u2<- .4 #correlation between R and u_t2
EffSize <- 0

population_sel <- declare_population(N = N, 
        u_t1 = rnorm(N) * sd_1, 
        u_t2 = gain_t1t2+rnorm(N, rho * scale(u_t1), sqrt(1 - rho^2)) * sd_2, 
        Y_t1 = u_t1, 
        L=rnorm(N, r_L.u2 * scale(u_t2), sqrt(1 - r_L.u2^2)) *  sd_2)

pops<-population_sel()
pops1<-potential_outcomes(pops)
assignment <- declare_assignment(Z = complete_ra(N))
pops2<-assignment(pops1) #added just for didactic purposes

attrition_rate<-.09 # from NFER report p 18
report <- declare_assignment(HasData = complete_ra(N, prob =  1-
                                               attrition_rate))
pops3<-report(pops2) #added just for didactic purposes
reveal_t2 <- declare_reveal(Y_t2)
pops4<-reveal_t2(pops3)
report2<-declare_assignment(Exclude = (L<0)&(Z==1))   #TRUE (ie exclude) if treated with below avg on L
pops5<-report2(pops4)

myancova_sel <- declare_estimator(Y_t2 ~ Z + Y_t1, .method = lm_robust, 
    inquiry = estimand, subset = (HasData==1 & Exclude==FALSE), label = "Excluding low level group 1")




NFER_design <- population + potential_outcomes + 
  estimand + assignment + reveal_t2 + report + 
  lm_pretest   #original NFER model as before

Ahmed_design <- population_sel + potential_outcomes + 
  estimand + assignment + reveal_t2 + report + report2+
  myancova_sel #contrasting Ahmed model includes report2 to retain only Top-Half GG


designs <- redesign(Ahmed_design,r_L.u2=c(0,.2,.4),EffSize=c(0,.2))
Ahmed_diagnosis<-diagnose_designs(designs,sims=nsims)
Ahmed_sims <- Ahmed_diagnosis$simulations_df #retain the simulations

tabAhmed <- Ahmed_diagnosis$diagnosands_df
tabxx <- t(tabAhmed)
ntab<-ntab+1
tabname<-paste0("Table ",ntab,": Diagnostic results from Ahmed model at two effect sizes, with different correlations between L and u_t2")

if(htmlout==1){
 knitr::kable(tabxx,escape = F, align = "c", booktabs = T,caption=tabname) %>%
   kable_styling(c("striped", "bordered"), latex_options = "striped", full_width = F)
}
if(htmlout==0){
print(tabxx)
}


```

```{r plotsims}
# plot simulations

plotsims<-function(myfile,myvar1,myvar2,longvar1,longvar2,mytitle){
c1<-which(names(myfile)==myvar1)
c2<-which(names(myfile)==myvar2)
myfile$myvar1<-paste0(longvar1,myfile[,c1])
myfile$myvar2<-paste0(longvar2,myfile[,c2])
mylabs<-c(myvar1,myvar2)
ggplot(myfile,aes(estimate)) + 
    geom_histogram(data=subset(myfile,sig == 1),fill = "red", alpha = 0.4) +
    geom_histogram(data=subset(myfile,sig == 0),fill = "blue", alpha = 0.4) +
    geom_vline(aes(xintercept = estimand),
             lty = "dashed", color = "#C6227F") +
    facet_grid(myvar1~myvar2) +
  labs(x = "Estimate", y = "Count of simulations") +
  ggtitle(mytitle)
}
```

```{r dofacetplots}

# myfile<-NFER_sims
# myvar1<-'rho'
# myvar2<-'EffSize'
# longvar1<-"Pre/post rho: "
# longvar2<-"Intervention effect size: "
# mytitle<-"NFER method: rho = .6 or .8"
# plotNFER<-plotsims(myfile,myvar1,myvar2,longvar1,longvar2,mytitle)
# 
# myfile<-Ahmed_sims
# myvar1<-"r_L.u2"
# myvar2<-"EffSize"
# longvar1<-"L/outcome r: "
# longvar2<-"Intervention effect size: "
# 
# mytitle <- "Ahmed method: rho = .6"
# plotAhmed<-plotsims(myfile,myvar1,myvar2,longvar1,longvar2,mytitle)

# myfile<-Ahmed_anova_sims
# myvar1<-"r_L.u2"
# myvar2<-"EffSize"
# longvar1<-"L/outcome r: "
# longvar2<-"Intervention effect size: "
# mytitle<- "Ahmed anova method: rho = .6"
# plot_Ahmedanova<-plotsims(myfile,myvar1,myvar2,longvar1,longvar2,mytitle)
# 
# #For direct comparison of Ahmed regression and anova can combine in one file and facet by method.
# 
# mycols<-intersect(names(Ahmed_sims),names(Ahmed_anova_sims))
# allAhmed<-rbind(Ahmed_sims[,mycols],Ahmed_anova_sims[,mycols])
# allAhmed$method='2 x 2 Anova'
# allAhmed$method[allAhmed$estimator=='Excluding low level group 1']<-'Covariate'
# 
# myfile<-allAhmed[allAhmed$estimand==0,]
# myvar1<-"r_L.u2"
# myvar2<-"method"
# longvar1<-"L/outcome r: "
# longvar2<-"Analysis: "
# mytitle<- "Ahmed analysis: pre/posttest rho = .6"
# plot_AhmedAll<-plotsims(myfile,myvar1,myvar2,longvar1,longvar2,mytitle)
# ggsave('AhmedAll.png')
```

What we can see in Table 4 and the plots above is that when the randomisation is broken and we select participants to include based on a variable (called here L, the level reached on GraphoGame) we introduce bias, or a systematic distortion of the true effect size. The degree of bias varies systematically with the extent to which L correlates with the level of reading ability at time 2. The higher that correlation, the greater the more the estimated effect size is (artifactually) increased. When the correlation is zero we obtain an unbiased (accurate) effect size. As the positive correlation gets larger, the estimated effect size gets larger.

It is important to emphasise that such correlations are almost always likely to occur, which is why conducting randomised experiments is so important -- because by definition in a randomised experiment we "break" any correlation that exists between participants' characteristics (in this case their propensity to learn to read) and the treatment they receive (GraphoGame versus no extra help). This basic idea has been accepted for centuries (Fisher and before...Bacon???) and it is why randomised experiments are so powerful. If you want to decide if an educational intervention works, if you can, do an randomised experiment to test its effects. If a randomise experiment shows a null result, any post hoc analyses of the data are fraught with difficulties. Avoid them at all costs, except as a prelude to another, randomised experiment.

We argue here that the conclusions from the analysis by Ahmed et al are insecure, because they used methods that introduce bias into estimates of effects of intervention - selection of subgroups for analysis after the study is completed. The problems created by such methods are well-known in fields such as clinical trials and political science. For instance, in their Guideline on the investigation of subgroups in confirmatory clinical trials, the Committee for Medicinal Products for Human Use (2019) stated: "From a formal statistical point of view, no further confirmatory conclusions are possible in a clinical trial where the primary null hypothesis cannot be rejected." And in a paper entitled: "How conditioning on post-treatment variables can ruin your experiment and what to do about it", Montgomery et al (2018) noted the dangers of practices such as "dropping participants who fail manipulation checks; controlling for variables measured after the treatment such as potential mediators; or subsetting samples based on post-treatment variables". All of these practices can lead to biased estimates. Unfortunately, the analyses conducted by Ahmed et al (2020) fall in this category. Here we use simulations to show how, under certain reasonable assumptions, the methods that they used are likely to have inflated the estimate of the intervention effect.

```{r savesims}


# 
# Ahmed_sims$sig <-0
# Ahmed_sims$sig[Ahmed_sims$p.value<.05]<-1
# 
# NFER_sims$sig <-0
# NFER_sims$sig[NFER_sims$p.value<.05]<-1
# 
# 
# save(Ahmed_diagnosis,file=paste0("Ahmed",nsims,".RData"))
# save(NFER_diagnosis,file=paste0("NFER",nsims,".RData"))

```

# References

Ahmed, H., Wilson, A., Mead, N., Noble, H., Richardson, U., Wolpert, M. A., & Goswami, U. (2020). An evaluation of the efficacy of Graphogame Rime for promoting English phonics knowledge in poor readers. Frontiers in Education, 5. https://www.frontiersin.org/articles/10.3389/feduc.2020.00132

Blair, G., Coppock, A., & Humphreys, M. (2023). Research Design in the Social Sciences: Declaration, Diagnosis, and Redesign. Princeton University Press. https://book.declaredesign.org

Goswami, U., & East, M. (2000). Rhyme and analogy in beginning reading: Conceptual and methodological issues. Applied Psycholinguistics, 21(1), 63--93. https://doi.org/10.1017/S0142716400001041

McTigue, E. M., Solheim, O. J., Zimmer, W. K., & Uppstad, P. H. (2019). Critically reviewing graphogame across the world: Recommendations and cautions for research and implementation of computer‐assisted instruction for word‐reading acquisition. Reading Research Quarterly, 55(1), 45--73. https://doi.org/doi:10.1002/rrq.256

Shadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal influence. Houghton, Mifflin and Company.

Worth, J., Nelson, J., Harland, J., Bernardinelli, D., & Styles, B. (2018). GraphoGame Rime: Evaluation report and executive summary. National Foundation for Educational Research. https://www.nfer.ac.uk/publications/graphogame-rime-evaluation-report-and-executive-summary/
