---
title: "When alternative analyses of the same data come to different conclusions:
  using DeclareDesign with a worked real-world example"
author: "Dorothy Bishop & Charles Hulme"
date: "26 Feb 2024"
output: word_document
format:
  docx:
    toc: no
    number-sections: no
editor: visual
---

Dorothy V. M. Bishop^1^\* & Charles Hulme^2^

^1^ Department of Experimental Psychology, University of Oxford, Anna Watts Building, Woodstock Road, Oxford, OX2 6GG.

^2^ Department of Psychology, Health and Professional Development, Oxford Brookes University, Oxford, OX3 0BP.

\*Corresponding author, [dorothy.bishop\@psy.ox.ac.uk](mailto:dorothy.bishop@psy.ox.ac.uk){.email}

```{=html}
<!---Online DeclareDesign book
https://book.declaredesign.org/declaration-diagnosis-redesign/declaration-in-code.html

see also
https://macartan.github.io/ci/syllabus.pdf-->
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=2)
library(knitr)
library(kableExtra)
library(gridExtra)
library(DeclareDesign)
library(tidyverse)
library(fabricatr)
library(randomizr)
library(estimatr)
library(nlme)
library(broom)
library(broom.mixed)
library(rstatix) #for convert_as_factor
library(reshape2)
library(ggpubr)
library(fishmethods) #for intracluster correlation
library(rdss) 
library(flextable)
ntab <- 0 #counter for labelling tables
nfig<-0
set.seed(1)

htmlout<-0 #1 for html. Have to avoid kable tables if you want Word (set myoutput to 2)
```

# Abstract

Worth et al. (2018) reported an independently conducted large-scale randomised controlled trial (RCT) to evaluate the effectiveness of GraphoGame, an intervention designed to enhance children's reading skills. They concluded the intervention did not improve children's reading (*g* = -.06) or spelling (*g* = .01). In contrast, a re-analysis by Ahmed et al (2020) concluded that the intervention significantly improved children's reading. We use a simulation package, DeclareDesign, to evaluate the impact on precision and bias of the analytic choices made in the reanalyses reported by Ahmed et al. The simulations showed that the decision to analyse a subset of data from better performing children in the original trial introduced substantial bias, overestimating the effect sizes. This problem was exacerbated by performing multiple separate analyses on correlated outcome measures. The combined effect of taking a subset of the data and failing to correct for multiple comparisons gives a false positive rate of around .35. We conclude that there is no evidence currently to demonstrate that GraphoGame is better than 'business as usual' classroom activities in improving children's literacy skills. This example demonstrates the dangers of performing reanalyses of data from RCTs that violate the randomisation of participants to conditions. More broadly, it showcases the value of simulation in contexts where different analyses of the same dataset have yielded different results. DeclareDesign helps researchers to quantify the precision and bias of estimates from different designs and optimise selection of measures and analytic methods.

# Alternative analyses: different conclusions

There are several studies in the social sciences that show that different researchers analysing the same data can come to different conclusions. Among the reasons for discrepancies are differences in specification of the research question (Kummerfeld & Jones, 2023), but even when a question is clearly defined, differences can emerge because of analytic decisions (e.g., Silberzahn et al., 2018; Schweinsberg et al., 2021). Bishop and Thompson (2024) noted that in randomised trials evaluating the effectiveness of interventions, we need to minimize the impact of random noise on our estimates, while also being alert to the possibility of methodological choices that can introduce systematic bias. Most researchers will attempt to address these issues by following well-established methods, but it is rare to see any systematic attempt to compare analytic approaches at the point of planning a study.

DeclareDesign is a suite of functions that allows one to simulate datasets from specified experimental designs and compare the effects of different sampling frames and analytic approaches (Blair, Coppock & Humphreys, 2023; <https://declaredesign.org/>). DeclareDesign was developed in the field of political science but is not widely used in other fields, despite its considerable benefits for researchers and policymakers. Simulations allow us to create datasets where the true values of estimated parameters are known, and then see how well those values are recovered in an analysis.

DeclareDesign adopts a formal approach to research design, distinguishing between four steps: Model, Inquiry, Data Strategy and Answer strategy (MIDA), which are combined to characterise a design, which can then be evaluated using simulated data. Blair, Cooper, Coppock and Humphreys (2019) noted that although simulated data is often used to estimate the statistical power of a design, it is seldom used to evaluate other properties, such as precision of estimates (as reflected in the root-mean-squared-error or RMSE), and bias (systematic differences between estimates and true values). Another index, coverage, combines aspects of precision and bias, reflecting the probability that the confidence interval around an estimate contains the true value. A useful feature of the package is that once a research design has been formally specified, it is easy to manipulate parameters to see the impact on statistical power, precision, bias, and other aspects of a proposed study.

In this article we illustrate how DeclareDesign can be used to compare different approaches to analysing data from an intervention trial. We take as a case study recent work evaluating results from a large randomised controlled trial (RCT) on the effectiveness of GraphoGame (<https://graphogame.com>), a computer-based game designed to improve children's reading. This example is pertinent because there are published analyses that lead to contradictory conclusions. The effectiveness of GraphoGame is a high stakes issue, given that, according to the company's website, it has been used by over 4 million children, and is an "effective, proven and affordable way to teach reading..." and "The most researched literacy game in the world." The simulations help to formalise some already generally well accepted guidelines for how to analyse randomized trials and in particular show that analyses that violate randomisation, by analysing non-representative subsets of data, are likely to introduce substantial bias.

# Background

GraphoGame is a set of adaptive online computer games designed to teach pupils to read (decode print) by developing their phonic reading skills (the ability to map letters in printed words onto words pronunciations). The games were initially developed at the University of Jyväskylä in Finland as a way of teaching children with dyslexia. Since then, GraphoGame has been adapted and translated into versions used in over 20 countries and a company markets the programme worldwide (<https://graphogame.com/>). The adaptations to GraphoGame made in different languages limit exact comparisons across studies because it is a learning platform and an instructional approach, rather than a single game with fixed properties. An English version, GraphoGame Rime, was developed to implement the Rhyme/Analogy theory of reading development proposed by Goswami and her colleagues (Goswami & East, 2000). This approach to reading is somewhat controversial and not consistent with the method mandated for use in primary schools in England which must use a Systematic Synthetic Phonics Program, in which children, from the beginning of learning to read, are taught to link individual letters in printed words to their corresponding phonemes (CAT -\> /k/ /a/ /t/). In GraphoGame Rime in contrast the initial emphasis is on teaching mappings at the level of rime units (the vowel and following consonant (if any) in a syllable (CAT -\> /k/ /at/)).

## Evidence for the effectiveness of GraphoGame.

GraphoGame is unusual in the field of education in that a number of randomised studies have been conducted to evaluate its effectiveness. A meta-analysis by McTigue et al. (2019) lists 15 randomised or quasi-experimental studies of GraphoGame (quasi-experiments are studies where students are not randomly assigned to conditions). McTigue et al. included a total of 19 independent comparisons in their meta-analysis. The majority of the studies had small sample sizes and none of the studies considered individually yielded statistically significant improvements in word reading as a result of GraphoGame. It has been pointed out that inclusion of studies with low power in meta-analyses is a potential source of bias due to the "file drawer problem"- underpowered studies that yield statistically significant results are more likely to be published than equivalent studies that yield null results. In fact, Kraemer, Gardner, Brooks, and Yesavage (1998) have argued forcefully that meta-analyses should exclude studies that are underpowered, as this will go a long way to removing the problem of misleading conclusions arising from the file drawer problem. However, nothwithstanding the fact that the majority of studies in the meta-analysis of McTigue et al., were underpowered, they reported an overall effect size for GraphoGame of almost exactly zero (*g* = -.02)

By far the largest randomised study of GraphoGame in the meta-analysis of McTigue et al. was a study conducted in England (Worth et al., 2018). This trial, registered at <http://www.isrctn.com/ISRCTN10467450>, selected poor readers who were randomly assigned within classrooms to GraphoGame Rime or a Business as Usual control group. The trial yielded an intervention effect size close to zero for the primary outcome measure (g = -0.06). This was a well conducted trial, although it suffered from a poorly chosen primary outcome measure (the New Group Reading Test) which is a complex blend of decoding and comprehension measures. A further problem was that the measure was at floor at pretest because it was too difficult for many of the children. However, effects were also null on a secondary outcome measure, a single word spelling test (g = 0.01) which arguably is better aligned with the GraphoGame treatment (since it is a more direct measure of phonic skills). The conclusion from this independently conducted evaluation was stark: *"The trial found no evidence that GraphoGame Rime improves pupils' reading or spelling test scores when compared to business-as-usual. This result has very high security"*. The high security rating refers to the Education Endowment Foundation's rating of the methodological quality of this randomised trial.

Ahmed et al. (2020) conducted a reanalysis of the data from this trial. In sharp contrast to the original report by Worth et al (2018), Ahmed et al. concluded that *"The current study suggests that young learners of the English orthography show significant benefits in learning both phonic decoding skills and spelling skills from the supplementary use of GG Rime in addition to ongoing classroom literacy instruction."* Those considering using Graphogame will find it confusing that such diametrically opposed conclusions can be drawn from analysis of the same dataset. Here we show how modeling the contrasting analytic approaches can clarify the sources of disagreement.

# Details of the original RCT

The project was funded jointly by the Wellcome Trust and the Education Endowment Foundation (EEF), as part of a scheme to develop collaborative intervention research between educators and neuroscientists. EEF projects undergo independent evaluation by researchers who are not involved in obtaining the funding. Details of the original RCT are provided in the report by independent evaluators from the National Foundation for Educational Research (Worth et al., 2018). All participants were selected as having low literacy skills, as assessed by the phonics screening check, a national assessment that is taken at the end of Year 1. GraphoGame Rime (GG) intervention was compared with business-as-usual in a sample of 398 Year 2 pupils from 15 primary schools. GraphoGame training occurred during literacy sessions, when the control group children received regular literacy activities with the class teacher. This was a two-armed pupil-randomised controlled trial powered to detect a minimal detectable effect size of .17. Final data were available for 362 children from two cohorts, each doing the intervention for one spring term in successive years. Allocation to intervention or control group was done by stratified randomisation of pupils within classroom, to ensure roughly equal numbers of children in intervention and control groups in each classroom. Attrition was around 10 per cent for both intervention and control groups, and did not appear biased but rather due to chance events such as absence on the day of the test.

GraphoGame Rime uses adaptive games, with the child progressing through different levels depending on their performance. Although the children should be supervised to ensure they can log on and remain on task, there is no active tuition by teachers. Game usage was remotely monitored, and the average playing time was six hours in the first cohort, and nine hours in the second cohort: the developer recommends that pupils should spend between 8.3 and 12.5 hours playing the game. The children in the intervention group played the game for 10-15 minutes each day during a literacy session, while those in the control group did other literacy activities.

The primary outcome was the raw score on the New Group Reading Test (NGRT), developed by GL Assessment, administered by testers from NFER within a month of the intervention ending (post-test). This same measure had been administered prior to intervention (pre-test). A spelling test was also administered at post-test.

The analysis used a single-level regression (ANCOVA) model. Raw score at post-test was the dependent variable, with intervention status (0 or 1), raw score at pre-test, and classroom (fixed effect) as predictors. The standardised effect size was the coefficient on the intervention group indicator, divided by the total sample standard deviation. Hedges' adjustment was applied for small sample sizes.

A planned subgroup analysis was conducted for the subsample of children who had free school meals (FSM) - an indicator of deprivation. Further analyses were conducted to consider how number of hours using GraphoGame related to outcome. The report noted, however, that *"Whilst this analysis appears attractive, it is very vulnerable to bias as those individuals who used the program the most are likely to have other characteristics that are associated with improved test performance"* - a point that is relevant to the subsequent analysis by Ahmed et al (2020) that is described below.

The evaluators noted that the correlation between pre-test and post-test scores on the reading test was only .57, lower than the anticipated value of .80. The primary analysis gave an estimated effect size of *g* = -.06 (97% CI -.23 to .12), i.e., the mean raw outcome score was marginally lower for the intervention group, but the difference was not reliably different from zero. Similar results were obtained with the spelling test (*g* = .01). The analysis with the subgroup of FSM children did not alter the findings. The interaction between pre-test score and intervention group was not statistically significant, indicating that the impact of intervention did not vary according to initial level of attainment. There was also no clear evidence that the intervention effect differed across classrooms, although the power to detect such effects was very low. Finally, there was a negative correlation between the amount of time spent playing GraphoGame and reading post-test scores (*r* = -.298): i.e., the more time the child spent playing the game, the less progress they made.

<!---p 24 We found a negative correlation between the amount of time pupils spent using GraphoGame and reading post-test scores: this suggests that pupils who spent longer playing the game were pupils who made less progress in reading. This difference also remains after taking account of pupils’ pre-test scores. This does not necessarily imply that spending more time on the game caused less progress as measured by the test scores. The amount of time spent using the game was a choice made by pupils and/or teaching staff, so was not randomly assigned and could reflect other underlying differences between the pupils/ teachers.-->

## Reanalysis by Ahmed et al (2020)

The reanalysis was headed by Usha Goswami from the University of Cambridge, the educational neuroscientist who had developed the English version of Graphogame Rime, and who had obtained the funding for the study. The Cambridge group provided training and technical support for the project, and their team had administered some additional measures to participating children.

Ahmed et al. (2020) reanalysed the data after dropping children from the intervention group who had not progressed beyond the mean level achieved on GraphoGame for the whole intervention group. These 95 intervention cases were then compared with the whole control group on a range of outcome measures. The rationale for this approach is that playing time was very variable, and some children may have used their time alone on the computer to do other activities. It was argued that it would be a fairer test to restrict consideration to children who had progressed far enough through the game to indicate that they had *"received sufficient independent and solitary exposure to the game to learn English phonics".*

According to this logic, one might have expected Ahmed et al. (2020) to focus on a subgroup of children who had spent some minimum amount of time playing the game. However, we already know from the NFER report that more time playing the game was not associated with better progress - if anything, the converse. Instead, the authors took the subset of children who made *most progress* through the game. The game is adaptive, progressing through 25 streams of phonic knowledge. The mean point reached by all the intervention group was level 5 of Stream 16. So the authors selected a "top half" group of children who played the game beyond that point. We will refer to this as the Top-Half GG group. This is a group of children, who based on their progress on GraphoGame, are mastering the rules of phonics better than the other children in the intervention group.

Repeated measures ANOVAs were run to compare progress in the top-half group with that of the control group on the original outcome measures (NGRT) as well as two subtests from the Test of Word Reading Efficiency (TOWRE; Torgesen et al, 1999). Such analyses are suboptimal in two ways compared to ANCOVA. First, they fail to account accurately for any pre-existing differences in baseline scores because they do not take account of the correlation between pretest and posttest scores, and second they have less power compared to ANCOVA (with the difference in power being proportional to the square root of the pretest-posttest measures' correlation).

Ahmed et al. (2020) reported analyses of five different measures. At posttest effects of GraphoGame on the New Group Reading Test, the TOWRE word reading subtest, and the TOWRE nonword reading subscale; at delayed follow-up, analysis of the TOWRE word and nonword tests. Out of these five analyses, one showed a significant effect of intervention: a significantly larger improvement in the GraphoGame group on the TOWRE nonword subtest (*p* = .027) at immediate posttest. Neither of the TOWRE subtests differed between the intervention and control group at the delayed subtest after the school summer holiday. No adjustment for multiple comparisons was made. 

Further analyses were conducted with subgroups of the Top-Half GG group versus subsets of controls, but we will not consider these further, as they will all be affected by any bias that arises from the initial subgrouping the sample on a post-intervention variable.

## Methods

We created simulated data to illustrate the impact of each of different analytic choices, using the DeclareDesign package (Blair, Coppock, & Humphreys, 2023) in the R computing language (R Core Team, 2023). We simulated the design reported by Worth et al. (2018) and that of Ahmed et al. (2020) to evaluate the effect of five differences in the analyses:

1)  *Reliability of outcome measures*. The NFER analysis used a preregistered outcome measure, the New Group Reading Test (NGRT). This proved to be problematic; the original plan was to use a version of the test (Level 2) that was too difficult, so Level 1B was substituted. As noted above, the reliability of this test was lower than anticipated. The Ahmed et al analysis used two subtests from the TOWRE as well as the NGRT.

2)  *Control for clustering by classroom*. The NFER analysis included a term in the regression analysis that took into account clustering of scores by classroom. This was not done in the Ahmed et al analysis.

3)  *Linear regression vs. repeated measures ANOVA*. The Ahmed et al analysis treated pretest and posttest readings scores as two levels of a dependent variable in repeated measures Analysis of Variance, whereas in the original analysis, linear regression was used with pretest as a covariate.

4)  *Use of multiple outcomes*. The NFER analysis focused on a single pre-registered outcome measure. The Ahmed et al analysis explored results from three related outcome measures.

5)  *Selection of participants for analysis*. The NFER analysis included all participants in the RCT for whom outcome data was available. The Ahmed et al analysis dropped half of the intervention group who were below average in the level of the game they had achieved, on the grounds that they may not have been engaged with the intervention.

For each analysis, we considered statistical power, bias, RMSE and coverage.

## Design specifications and results

### Design 1. Using DeclareDesign to simulate original analysis

We started by simulating data from the original trial, which adopted a fairly standard approach for analysing a Randomised Controlled Trial (Shadish, Cook, & Campbell, 2002), where children were randomly assigned to Intervention or Control groups. ANCOVA (regression) analysis used to estimate the effects of the intervention on a preregistered outcome measure, with a pre-test score used as a covariate.

DeclareDesign disinguishes between four steps: Model, Inquiry, Data Strategy and Answer Strategy (MIDA), which are combined to characterize a design. This can then be evaluated using simulated data. The DeclareDesign package includes a Design Library, with R code for simulating common experimental designs (see <https://declaredesign.org/r/designlibrary/>). We took as our starting point the Pretest Posttest Design (<https://declaredesign.org/r/designlibrary/articles/pretest_posttest.html>), which implements the type of analysis used by Worth et al. (2018). Although the simulation can be run as a single step with specified parameters, we will walk through each step of code here to illustrate the MIDA approach. A potentially confusing feature of DeclareDesign is that the various 'declare' functions do not compute results, but rather specify new functions. These functions are then combined to create a full design specification. Here we have added some additional steps to the code to make it possible to inspect what is achieved in the different steps, and some variable names have been changed to make them more aligned with terminology commonly used in psychology.

<!--code for DeclareDesign is here: https://dataverse.harvard.edu/file.xhtml?fileId=7017490&version=5.2-->

#### Model

The model specifies the nature of the sample and observed variables, plus the estimated impact of an experimental manipulation (in this case intervention). For simplicity, we simulate scores as normally distributed with a standard deviation of 1. We need to specify the sample size (prior to attrition), *N*, the correlation between pretest and posttest scores, *rho*, and the size of the intervention effect or *EffSize* (In the DeclareDesign examples, this is referred to as *ate* or *Average Treatment Effect*). In addition, since Worth et al. (2018) reported general improvement in scores on the reading test from time 1 to time 2 of around 1 SD, regardless of intervention, this is added to time 2 scores. The Model 1 Specification code achieves these steps. The first eight rows of the resulting simulated data are shown in Table 1.

```{r loaddiagnoses, echo=FALSE}
#For demonstration of how script works, set haveresults to zero here.
#Otherwise, later chunks will preload runs that were created with 10K simulations for all models.
#This saves time if you want to plot or tabulate data from final simulations.

haveresults<-1  #default is zero. 
#if models have already been run then we load them in rather than rerunning them



```

```{r specifymodel}
# ------------------------------------------------------
# DESIGN 1: MODEL SPECIFICATION
# ------------------------------------------------------

N         <- 398 #full sample size, prior to attrition
sd_1      <- 1 #we work with random normal deviates, so SD = 1
gain_t1t2 <- 1 #scores improve by 1 SD on average  
sd_2      <- 1 #we work with random normal deviates, so SD = 1
rho       <- 0.6 # correlation between time 1 and time 2 scores (Table 3 of NFER report = .57)
EffSize   <- 0 # average treatment effect (Table 6 of NFER report = -0.06)
nsims     <- 10 #number of simulations: set to around 50 for testing, but to a high number for final run

population <- declare_population(
  N = N, 
  u_t1 = rnorm(N) * sd_1, 
  u_t2 = gain_t1t2 + rnorm(N, rho * scale(u_t1), sqrt(1 - rho^2)) * sd_2, 
  Y_t1 = u_t1)

mypop <- population() #this step is added just to make output of function visible for explanatory purposes
potential_outcomes <- declare_potential_outcomes(Y_t2 ~ u_t2 + 
    EffSize * Z) #defaults to 2 conditions of intervention variable Z, with values 0 or 1
mypot <- potential_outcomes(mypop) #this step is added just to make output of function visible for explanatory purposes
```

```{r savetab1, echo=FALSE}
with the pre#Save output in a table
demotab<-mypot[1:8,]
ntab<-ntab+1 #increment table counter
tabname<-paste0("GGTables/Table ",ntab,"_Simulated potential outcomes for Model 1.csv")

write.csv(demotab,tabname,row.names=F)

ftab<-flextable(demotab)
ftab<- fit_to_width(ftab,9) #convert from table1 to flextable format to allow formatting of width etc
mycap<-paste0("Table ",ntab,": Simulated potential outcomes for Design 1")
ftab<-set_caption(ftab,mycap)
ftab

```

The first function, *population*, generates the first four columns for 398 rows. The first column, *ID*, identifies each simulated participant by a sequential number. Columns with the prefix u\_ correspond to unobserved latent variables, with u_t1 representing time 1 scores and u_t2 representing time 2 scores. The code specifies that these are drawn from a population where u_t1 and u_t2 are correlated with correlation *rho*. For the whole dataframe, the mean of u_t2 is greater than u_t1 by the value *gain_t1t2*. Columns with the prefix Y\_ correspond to expected values for observed variables. Y_t1 is the same as u_t1 and corresponds to the observed pretest value. Although this is redundant, it clarifies the distinction between unobserved, latent variables and observed variables. For Y_t2 there are two values generated, Y_t2_Z_0 and Y_t2_Z_1. These are expected observed values at posttest (potential outcomes), depending on whether the case is allocated to the control group (Z_0) or the intervention group (Z_1). The values of Y_t2_Z_0 are the same as values of u_t2, whereas the values of Y_t2_Z_1 correspond to u_t2 plus the value of *EffSize*. That is, Y_t2_Z_1 corresponds to the expected observed value at posttest plus an added component equal to the average treatment effect.

#### Inquiry

The inquiry specifies what parameter we want to estimate from the Design - known as the estimand. This could be a descriptive statistic, such as the mean value of a variable, or the difference or correlation between variables. Here we specify the mean difference between intervention and control groups at time 2 as the estimand.

```{r specifyinquiry}
# ------------------------------------------------------
# DESIGN 1: INQUIRY SPECIFICATION
# ------------------------------------------------------
estimand <- declare_inquiry(EffSize = mean(Y_t2_Z_1 - Y_t2_Z_0))
```

If we apply the estimand function to the data frame generated at the Model stage, we get a value of the estimand of `r estimand(mypot)$estimand`. While this is reassuring, it is hardly surprising, since we defined Y_t2_Z_0 and Y_t2_Z_1 as equivalent but with the specified *EffSize* added to the Y_t2_Z_1 condition. However, in subsequent steps we can consider how the estimand compares with estimates of its value in subsets of simulated data, with differences between the estimand and estimates indicating how much bias there is in the analysis. In addition, we can use inquiries for descriptive statistics, as illustrated in the code below.

```{r otherinquiries}
# ------------------------------------------------------
# ILLUSTRATION OF OTHER INQUIRIES
# ------------------------------------------------------
mean_t1 <- declare_inquiry(mean_t1 = mean(Y_t1)) #Not used in the Design: added to demonstrate use of declare_inquiry for descriptive statistics
cor12 <- declare_inquiry(cor_t1t2=cor(u_t1,u_t2)) #Not used in the Design: added to demonstrate use of declare_inquiry for descriptive statistics

```

These are not part of the main pretest_posttest design function, but may be useful when checking the simulation against real data. The value of mean_t1 and cor_t1t2 are likely to differ from expected values of 0 and rho respectively, because they are obtained by sampling cases from a population, and so will vary with each run of the simulation. In this run of the simulation, mean_t1 is `r mean_t1(mypot)$estimand` and cor12 is `r cor12(mypot)$estimand`.

#### Data strategy

The data strategy step selects data for allocation to treatments and for analysis. In the chunks of code below, we first use *declare_assignment* to randomly assign cases to 0 (control) or 1 (intervention), and then use it again to specify whether or not the case *HasData* (using the attrition rate to randomly assign a proportion of cases as HasData = 0). We also make a new column that corresponds to the outcome corresponding to the intervention assignment for each case, and finally, we compute the observed difference *(diff_t1t2*) between posttest and pretest scores for each row. The first 8 rows of resulting simulated data from this code are shown in Table 2.

```{r datastrategy}
# ------------------------------------------------------
# DESIGN 1: DATA STRATEGY
# ------------------------------------------------------
assignment <- declare_assignment(Z = complete_ra(N))
mypota<-assignment(mypot) #added just for didactic purposes

attrition_rate<-.09 # from NFER report p 18
report <- declare_assignment(HasData = complete_ra(N, prob =  1-
                                               attrition_rate))
mypotb<-report(mypota) #added just for didactic purposes
reveal_t2 <- declare_reveal(Y_t2)
mypotc<-reveal_t2(mypotb)
manipulation <- declare_step(diff_t1t2 = (Y_t2 - Y_t1), 
                             handler = fabricate)
mypotd<-manipulation(mypotc)
```

```{r savetab2, echo=FALSE}
#Save output in a table
demotab<-mypotd[1:8,]
ntab<-ntab+1 #increment table counter
tabname<-paste0("GGTables/Table ",ntab,"_Simulated potential outcomes for Design 1.csv")

write.csv(demotab,tabname,row.names=F)

ftab<-flextable(demotab)
ftab<- fit_to_width(ftab,10) #convert from table1 to flextable format to allow formatting of width etc
mycap<-paste0("Table ",ntab,": Simulated data with assignment to intervention for Design 1")
ftab<-set_caption(ftab,mycap)
ftab

```

The simulated data frame now has additional columns. Column Z indicates whether the individual is assigned to control (0) or intervention (1) condition. *HasData* is 1 for most cases, and 0 for around 9%. The *Y_t2* column is created by selecting *Y_t2_Z_0* where *Z* is 0, and *Y_t2_Z_1* where *Z* is 1. Although potentially we could remove cases where *HasData* = 0 by setting *Y_t2* to NA, this effect is achieved later on, at the Answer step, where we use this variable to specify which data to include. The final column is the observed difference between scores at t2 and t1. The latter is not used for our current analysis, but will feature at a later stage.

#### Answer strategy

The answer strategy involves first fitting a statistical model to data, and then summarising the model fit. The example of the *pretest_posttest* function in the DeclareDesign vignette compares three different analytic approaches, all implemented in linear models. For now, we will focus just on an analysis corresponding to that adopted in the Worth et al. (2018) report; predicting outcome (*Y_t2*) from two variables: treatment group (*Z*) and pretest score (*Y_t1*). Worth et al. added classroom as a further covariate, but for this initial demonstration, we will ignore that.

```{r answerstrategy}
# ------------------------------------------------------
# DESIGN 1: ANSWER STRATEGY
# ------------------------------------------------------
lm_pretest <- declare_estimator(Y_t2 ~ Z + Y_t1, .method = lm_robust, 
                                 inquiry = estimand, subset = HasData == 1, label = "Pretest as covariate")

#  Note that we use the subset function to restrict analysis to those where HasData == 1

#  The output of this is same as for Z term with this syntax:
#  mylm <- lm_robust(Y_t2 ~ Z + Y_t1,mypotd[mypotd$HasData==1,])


```

```{r savetab3, echo=FALSE}
#Save output in a table

tablm <- lm_pretest(mypotd)
ntab<-ntab+1 #increment table counter

tabname<-paste0("GGTables/Table ",ntab,"_Output from linear model applied to simulated data.csv")
write.csv(tablm,tabname,row.names=F)

ftab<-flextable(tablm[,-c(1,7,8)])
ftab<- fit_to_width(ftab,8) #convert from table1 to flextable format to allow formatting of width etc
mycap<-paste0("Table ",ntab,": Output from linear regression applied to one run of simulated data")
ftab<-set_caption(ftab,mycap)
ftab

```

Table `r ntab` shows the effect of running the *lm_pretest* function on a single simulated dataset. The 'statistic' here is a t-value that tests the statistical significance of the prediction of outcome from intervention and pre-test score.

To get a reliable estimate of the properties of this design, and the variability around the estimated parameters, we need to run the simulation many times. We first create our full design by bolting together the elements of the Model, Inquiry, Data Strategy and Answer Strategy, and then run *diagnose_design* with a specified number of simulations. We can also use the *redesign* function to easily change the values of parameters and rerun the simulation to see the effect. Here we compare results when the true effect size is 0, with the same Design when the true effect size is .2. In addition, we consider how varying the test-retest correlation between the pretest and posttest measure affects results, by comparing results when *rho* is set to .6 vs .8. This is pertinent to the current example, given that the reliability of the outcome measure used by Worth et al. (2018) was lower than predicted.

```{r declaredesign,warning=FALSE}
# ------------------------------------------------------
# DESIGN 1: DECLARE THE DESIGN
# ------------------------------------------------------
   
Design1 <- population + potential_outcomes + estimand + assignment + reveal_t2 + report + 
  lm_pretest

#We will compare Designs with two values of EffSize and two values of rho
#Original power computation used EffSize of .17, here we select .2

designs <- redesign(Design1, EffSize = c(0,.2), rho = c(.6,.8))

Design1_diagnosis <- diagnose_designs(designs, sims=nsims)

save(Design1_diagnosis,file=paste0("simulated_data/Design1_",nsims,".RData")) #we save the simulations and diagnosands as R.Data. Nsims saved as affix to name.

```

```{r savetabDesign1diag,echo=FALSE, warning=FALSE}
ntab<-ntab+1
#we use prior simulated data if we have them
if(haveresults==1){
  load("simulated_data_10K/Design1_10000.RData")
}
tabDes1 <- Design1_diagnosis$diagnosands_df
tabname<-paste0("GGTables/Table ",ntab,"_Diagnosis of properties of Design 1 with different values of EffSize and rho.csv")
write.csv(tabDes1,tabname,row.names=F)

#simplify table for display
tabDes1a<-tabDes1[c(1,3,2,4),c(2,3,10,11,16,18,20)]
ftab<-flextable(tabDes1a)
ftab<- fit_to_width(ftab,10) #convert from table1 to flextable format to allow formatting of width etc
mycap<-paste0("Table ",ntab,":  Diagnostic results from Design 1 (NFER) with different parameter settings, 10,000 simulations")
ftab<-set_caption(ftab,mycap)
ftab
nfig <- nfig+1
```

The default output from the model diagnosis is complex, and we present here simplified tables that just show the parameters varied in the Design, and the mean estimate of intervention effect with its standard error, the RMSE, the statistical power, and the coverage (probability that the confidence interval contains the true value). An optimal design will give estimates close to the true value (estimand), with RMSE below .1, and coverage around .95. The proportion of significant estimates is labelled as power. Where the true effect size is greater than zero, power indicates the probability of detecting a genuine effect given the research design. If, however, the true effect size is zero, then this value corresponds to the false positive rate, and is expected to be close to .05 in that case. Figure `r nfig` shows the distribution of estimates from the model after 10,000 runs of each simulation with four different parameter settings, and Table `r ntab` shows the corresponding simplified output from the diagnosis. Note that in these simulations, all tests are two-tailed; if we were interested only in positive estimates, the false positive rate would be halved.

(Figure `r nfig` about here)

*Figure `r nfig`*. Simulated distributions of estimated effect size for Design 1, 10,000 simulations. The blue corresponds to nonsignificant estimates, and the purple to estimates where p \< .05. For the null case, where the true intervention effect size is zero, the purple regions correspond to the false positive rate on a two-tailed test.

There are two notable effects shown in Figure `r nfig`. First, the simulated data generate the "true" effect sizes accurately (mean effect sizes of 0.0 and 0.2 as expected). Second an increase in the pretest-posttest correlation does not affect the estimated effect size, but it does increase power (the variability in the estimates of the effect size are smaller when the correlation is higher).

### Design 2: Modifying the simulation to incorporate classroom as a cluster

Intervention studies conducted in schools can be affected by clustering, if children within a classroom are more similar to each other than to children from different classrooms.  Typical standard errors and p-values from a regression model are based on the assumption that the residuals in the model across cases are independent from each other.  If children within classrooms are more similar to each other, this violates the assumption of the independence of residuals, and to the extent to which that is true standard errors will be underestimated (and *p*-values over-estimated). Children from 53 classrooms were included in the trial, with an average of 7.5 pupils per class eligible for the trial, and 3.8 pupils per class randomised to receive the intervention. There is much debate about the best way to take such effects into account: Worth et al. (2018) modelled classroom as a fixed effect, so we next modified the simulation to include this variable.

The key statistic for any clustering analysis is the intra-cluster correlation (ICC) between the cluster variable and pretest. We explore the effects of a varying ICC with the pretest NGRT score, with values of .15 and .30. In the simulation, we first allocate a classroom to each child, and then generate *u_t1 (the distribution of the unobserved latent variable representing the pretest scores)* to conform to size of the ICC for classrooms.

```{r makeclustered}

# ------------------------------------------------------
# DESIGN 2: MODEL SPECIFICATION
# ------------------------------------------------------

# Simulate the number of children in each of 53 classes
nclasses <- 53
M <- declare_model(
  id = add_level(
    N = 398,
    class = sample(1:nclasses,N,replace=TRUE)
  )
)
mypop<-M() #can look at this to see the allocation.
#Next we need to add variables as before, but u_t1 needs to be clustered with class


estICC <- .15 #estimate of intracluster correlation between cluster and pretest NGRT
# (we vary this later to see the effect)

M2 <-
  declare_population(
                  u_t1 = draw_normal_icc(
                    mean = 0,
                    clusters = class,
                    ICC = estICC 
                  ),
                  u_t2 = gain_t1t2 + rnorm(N, rho * scale(u_t1), sqrt(1 - rho^2)) * sd_2, 
                  Y_t1 = u_t1)
                
nupop<-M2(mypop)

#check clustering - this function from fishmethods confirms we recreated correct ICC
clusrho <- clus.rho(popchar=nupop$u_t1, cluster = nupop$class, type = 3, est = 0, nboot = 500)
```

The Inquiry and Data Strategy are the same as for Design 1. We need to modify the Answer Strategy to take clustering into account.

```{r des2ans}
# ------------------------------------------------------
# DESIGN 2: ANSWER STRATEGY
# ------------------------------------------------------
#Now we modify the regression equation to include the classroom as cluster
lm_pretest_cluster <- declare_estimator(Y_t2 ~ Z + Y_t1, .method = lm_robust, cluster=class, fixed_effects= ~class,inquiry = estimand, subset = HasData == 1, label = "Pretest as covariate, clustered")

```

The design declaration is the same as for Design 1, except we substitute M and M2 in the Model, and use lm_pretest_cluster in the Answer strategy.

```{r Design2NFERcluster,warning=FALSE}
# ------------------------------------------------------
# DESIGN 2: DECLARE THE DESIGN
# ------------------------------------------------------
Design2 <- M + M2 + potential_outcomes + estimand + assignment + reveal_t2 + report +  lm_pretest_cluster   

#This time we also explore how different values of ICC affect results
designs          <- redesign(Design2,EffSize=c(0,.2),rho=c(.6,.8),estICC=c(.15,.3))
Design2_diagnosis<-diagnose_designs(designs,sims=nsims)
Design2_sim      <- Design2_diagnosis$simulations_df #saves a dataframe of all the simulation runs
Design2_diags   <-Design2_diagnosis$diagnosands_df

save(Design2_diagnosis,file=paste0("simulated_data/Design2_",nsims,".RData"))
```

```{r savetabDes2diag,echo=FALSE, warning=FALSE}


#we use prior simulated data if we have them
if(haveresults==1){
  load("simulated_data_10K/Des2_NFERcl_10000.RData")
}
tabDes2 <- Des2_NFER_cl_diagnosis$diagnosands_df
ntab<-ntab+1
tabname<-paste0("GGTables/Table ",ntab,"_Diagnosis of properties of Design 2 with different values of EffSize,  rho and ICC.csv")
write.csv(tabDes2,tabname,row.names=F)

#simplify table for display
mycols <- c('EffSize','rho','estICC','mean_estimate','se(mean_estimate)','rmse','power','coverage')
tabDes2a<-tabDes2[,mycols]
names(tabDes2a)[3]<-'ICC_cluster'

tabDes2a<-tabDes2a[with(tabDes2a,order(EffSize,rho,ICC_cluster)),]

ftab<-flextable(tabDes2a)
ftab<- fit_to_width(ftab,8) #convert from table1 to flextable format to allow formatting of width etc
mycap<-paste0("Table ",ntab,":  Diagnosis of properties of Model 2 with different values of EffSize,  rho and ICC")
ftab<-set_caption(ftab,mycap)
ftab
```

The results in Table `r ntab` confirm that the clustering by classroom has a negligible effect on the analysis, even when the ICC for clusters level is relatively high. It would be simple to explore the impact of different cluster sizes by changing the value of *nclasses* in the model specification.

### Design 3: Comparing repeated measures ANOVA with ANCOVA

Ahmed et al. (2020) treated pretest and posttest scores as different levels of a dependent variable in an Analysis of Variance, where group was the between-subjects factor and time was the within-subjects factor. The intervention effect is then estimated from the interaction term. Mathematically, this analysis is equivalent to a group comparison of the time 2 minus time 1 difference scores for the two groups. We can therefore readily check the impact of this analytic decision using code that is included in the *pretest_posttest* function in the DeclareDesign vignette.(<https://declaredesign.org/r/designlibrary/articles/pretest_posttest.html>). This time our Design includes two alternative analyses of the same simulated data - the original regression with pretest as covariate, and the analysis of difference scores. Note that once the Design is set up, it is trivially easy to incorporate this additional comparison in DeclareDesign. The Model, Inqiry and Data Strategy are unchanged; we just add a new term to the Answer Strategy.

```{r changescore, warning =FALSE}
# ------------------------------------------------------
# DESIGN 3: ANSWER STRATEGY
# ------------------------------------------------------
changescore <- declare_estimator(diff_t1t2 ~ Z, .method = lm_robust, 
    inquiry = estimand, subset = HasData == 1, label = "Change score")

```

The declaration of the design is identical to Design 1, except that we now include two different answer strategies: the original *lm_pretest* (ANCOVA) and the new *changescore* (repeated measures ANOVA). Our diagnosis will then show both methods compared using the same simulated data.

```{r declaredes3}
# ------------------------------------------------------
# DESIGN 3: DECLARE DESIGN
# ------------------------------------------------------
Design3 <- population + potential_outcomes  + estimand + assignment + reveal_t2 + report  + manipulation + lm_pretest + changescore

diagnosis <- diagnose_design(Design3,sims=nsims)

designs_diff <- redesign(Design3,EffSize=c(0,.2),rho=c(.6,.8))
Design3_diagnosis<-diagnose_designs(designs_diff,sims=nsims)

save(Design3_diagnosis,file=paste0("simulated_data/Design3_",nsims,".RData"))
```

```{r savetabMod3diag,echo=FALSE, warning=FALSE}
ntab<-ntab+1
#we use prior simulated data if we have them
if(haveresults==1){
  load("simulated_data_10K/Mod3_NFERdiff_10000.RData")
}
tabMod3 <- Mod3_diff_diagnosis$diagnosands_df
tabname<-paste0("GGTables/Table ",ntab,"_Comparison of ANCOVA (regression) vs repeated measures (change scores) analysis.csv")
write.csv(tabMod3,tabname,row.names=F)

#simplify table for display
tabMod3a<-tabMod3[,c(5,2,3,10,11,16,18,20)]
ftab<-flextable(tabMod3a)
ftab<- fit_to_width(ftab,8) #convert from table1 to flextable format to allow formatting of width etc
mycap<-paste0("Table ",ntab,":  Comparison of linear regression vs repeated measures (change score) analysis")
ftab<-set_caption(ftab,mycap)
ftab
```

This analysis confirms that precision is greater and power is higher for the regression analysis than for the repeated measures ANOVA when there is a true intervention effect, but the difference is slight.

### Design 4: Including multiple outcomes

It is well-established that the false positives will increase if several outcomes are analysed without any statistical correction. Nevertheless, the standard recommendation to have a single outcome measure is not necessarily optimal in educational trials, where there may be a range of outcome measures that might be expected to be influenced by the intervention. . Here we show how DeclareDesign can be used to simulate the situation when there are three correlated outcome measures.

To generate correlated measures in the simulation we created a correlation matrix for three measures taken at both pre- and post-test. The test-retest correlation for each measure is set at .7, the intercorrelation between different measures taken on the same occasion is set at .5, and the intercorrelation between different measures on different occasions is set at .4. We also include a correlation of .2 between all measures and a 'Level' index, which will be explained when we come to Design 6, but which for now is ignored. We use the *mvrnorm* function from the *MASS* package in R to generate data with the specified correlations.

```{r Design4multivar, warning=FALSE}
# ------------------------------------------------------
# DESIGN 4: MODEL SPECIFICATION
# ------------------------------------------------------
mycors<-read.csv('cormatrix_sim.csv') #correlation matrix for 3 measures on 2 occasions, with test-retest correlation set to .7, intercorrelation between different measures on same occasion set to .5, and intercorrelation between different measures on different occasions set to .4. We also include a Level measure which correlates .2 with all other measures (see later). 

mynames<-mycors[,1]
mycormat<-as.matrix(mycors[,2:ncol(mycors)])
Mod4 <-
  declare_model(
    N = 398,
    draw_multivariate(c('u_t1a','u_t1b','u_t1c','u_t2a','u_t2b','u_t2c','Level')~ MASS::mvrnorm(
      n = 398,
      mu = c(0,0,0,1,1,1,0), #time 2 reading outcomes have effect size of 1 added
      Sigma = mycormat)
    ))
mypop <- Mod4() #this step is added just to make output of function visible for explanatory purposes
#NB mypop names start with X. and end with .
```

The code for Model, Inquiry, Data Strategy and Answer Strategy for Design 4 from this point is equivalent to that for Design 1, except that we create potential outcomes, estimands, observed outcomes, and estimators separately for each of the three reading variables. (The full code is available on OSF).

```{r moreMod4}


#potential outcomes - tried doing this in one command but it did not work, so separate command for each step in sequence
potential_outcomes_a <- declare_potential_outcomes(Y_t2a ~ X.u_t2a. + EffSize * Z)
potential_outcomes_b <- declare_potential_outcomes(Y_t2b ~ X.u_t2b. + EffSize * Z)
potential_outcomes_c <- declare_potential_outcomes(Y_t2c ~ X.u_t2c. + EffSize * Z)

##defaults to 2 conditions of variable Z, with values 0 or 1

mypota <- potential_outcomes_a(mypop) #this step is added just to make output of function visible for explanatory purposes
mypotb<-potential_outcomes_b(mypota)
mypotc<-potential_outcomes_c(mypotb)


estimanda <- declare_inquiry(
  EffSize_a = mean(Y_t2a_Z_1 - Y_t2a_Z_0))
estimandb <- declare_inquiry(
  EffSize_b = mean(Y_t2b_Z_1 - Y_t2b_Z_0))
estimandc <- declare_inquiry(
  EffSize_c = mean(Y_t2c_Z_1 - Y_t2c_Z_0))

#All estimands are zero

#datastrategy}

assignment <- declare_assignment(Z = complete_ra(N))

attrition_rate<-.09 # from NFER report p 18
report <- declare_assignment(HasData = complete_ra(N, prob =  1-
                                               attrition_rate))

reveal_t2a <- declare_reveal(Y_t2a)
reveal_t2b <- declare_reveal(Y_t2b)
reveal_t2c <- declare_reveal(Y_t2c)

#Use the corresponding pretest measure for the 3 measures with pre and post- 
lm_pretest_a <- declare_estimator(Y_t2a ~ Z + X.u_t1a., .method = lm_robust, 
                                 inquiry = estimanda, subset = HasData == 1, label = "A_Pretest as cov")
lm_pretest_b <- declare_estimator(Y_t2b ~ Z + X.u_t1b., .method = lm_robust, 
                                 inquiry = estimandb, subset = HasData == 1, label = "B_Pretest as cov")
lm_pretest_c <- declare_estimator(Y_t2c ~ Z + X.u_t1c., .method = lm_robust, 
                                 inquiry = estimandc, subset = HasData == 1, label = "C_Pretest as cov")
```

We then specify Design4, which includes all three variables (here labelled as a, b and c).

```{r Design4design}
# ------------------------------------------------------
# DESIGN 4: SPECIFY DESIGN
# ------------------------------------------------------
Design4 <- Mod4 + potential_outcomes_a + potential_outcomes_b + potential_outcomes_c + 
  estimanda + estimandb + estimandc + assignment + reveal_t2a + reveal_t2b + reveal_t2c + report + 
  lm_pretest_a + lm_pretest_b + lm_pretest_c  

Design4_designs <- redesign(Design4,EffSize=c(0,.2)) #Here we just look at 2 effect sizes

Design4_diagnosis<-diagnose_designs(Design4_designs,sims=nsims)
save(Design4_diagnosis,file=paste0("simulated_data/Design4_",nsims,".RData"))
```
For this model, DeclareDesign yields diagnosands that are similar to those in Design 1. Bias is introduced, however, if the researcher then selects any of the set of outcome measures as evidence for effectiveness. When modeling this situation, we have to decide what kind of selection might be made. Potentially, the researcher might be interested in any result that is statistically significant. However, that seems implausible, because it would include negative as well as positive results. In our simulation, we assume that the researcher selects the largest positive effect size, and disregards results that are opposite to prediction. To extract that information, we extract the simulations that are stored as part of the Diagnosis of Design4, and for each run select the variable giving the most extreme positive result. 

```{r Des4analysis, echo=F}
if(haveresults==1){
  load("simulated_data_10K/Design4_10000.RData")
  Design4_diagnosis<-Mod4_mult_diagnosis
}

Des4sims <- Design4_diagnosis$simulations_df #retain the simulations
#classify results as significant or not


Des4null<-Des4sims[Des4sims$EffSize==0,] #we are just interested in cases where Null hypothesis is true, to compute in False positive rate
nsimsx <- max(Des4null$sim_ID)

#make new surrogate simulation df with nsims rows
newsim4<-Des4null[1:nsimsx,] #we will overwrite these values with var giving largest effect

for (i in 1:nsimsx){
    w<-which(Des4null$sim_ID==i) #find rows for this run
    maxw<-which(Des4null$estimate[w]==max(Des4null$estimate[w])) #largest estimate, so one-tailed
    newsim4[i,]<-Des4null[w[maxw],]
}

newsim4$sig <- 0
wx <- intersect(which(newsim4$p.value<.05),which(newsim4$estimate>0))
newsim4$sig[wx] <- 1
wantcols<-c('estimate','sig')
w<-which(names(newsim4) %in% wantcols)
newsim4summary<-colMeans(newsim4[,w])
fprate<-newsim4summary[2]
ntab <- ntab+1

showtab <- newsim4[1:8,c('sim_ID','inquiry','estimate','std.error','p.value','sig')]
ftab<-flextable(showtab)
ftab<- fit_to_width(ftab,8) #convert from table1 to flextable format to allow formatting of width etc
mycap<-paste0("Table ",ntab,":  Most extreme positive results from the first 8 simulations of Design 4")
ftab<-set_caption(ftab,mycap)
ftab
```

The first 8 rows of simulation results are shown in Table `r ntab` for the case where the null hypothesis is true (EffSize = 0). The inquiry indicates which of the three measures, a, b, and c, was selected as most extreme. If the three measures were uncorrelated, then each measure would have an independent probability of showing a positive significant effect (p < .05) of .025, and so the estimated false positive rate would be 1 - .976^3, which is equivalent to .07. However, the more highly correlated measures are, the less the false positive rate will be inflated. We can categorise the p-value of the most positive result on each run as either significant (p < .05) or not, to compute the familywise false positive rate. For outcomes with the correlation structure used here, with significance counted only for positive effects, the probability of a false positive (i.e., a p-value below .05 on at least one of three measures) is only slightly inflated, at `r fprate`. 

### Design 5: Modifying the selection of cases to match the Ahmed et al analyses

It is relatively straightforward to extend the design to represent the selection of the Top-Half GG cases as was done by Ahmed et al. (2020). To do this we create a new latent variable, L, which represents the level reached by players of GraphoGame. This is modeled as a random normal deviate, as for the other latent variables. A critical issue is how far it correlates with u_t2 (the level of the unobserved latent reading ability at time 2). It seems  likely that the children who progress furthest through the game are those that learn fastest on GraphoGame, which after all is a measure of learning to read (learning correspondences between letters and groups of letters in printed words, and their pronunciations). Ahmed et al (2020) assume that this happens because practice on the game causes better reading ability, but this direction of causality cannot be assumed: there are many reasons why some children learn faster than others which could influence both the outcome measure and the rate of progress through the game. It could be that children who learn faster on the game are those with a higher propensity to learn to read well (for reasons not specified); in other words, selecting children based on the level attained on the GG game, may be artifactually selecting children who are going to learn to read best, whatever teaching they receive. Such an effect may have nothing to do with the time spent on GraphoGame. With randomised assignment, we can get a handle on causality, but once we break the randomisation, this is no longer the case. In effect, the selection method adopted by Ahmed et al focuses on children in the intervention group who did well, using a proxy measure (level of game attained) which is not random.

We can model this scenario with a new variable, r_L.u2, which represents the correlation between L (the Level reached on GraphoGame) and u_t2 (the unobserved latent reading ability at time 2). 


```{r Design5model,  warning=F}
# ------------------------------------------------------
# DESIGN 5: SPECIFY MODEL
# ------------------------------------------------------
r_L.u2<- .2 #correlation between Level attained and outcome (u_t2)
EffSize <- 0

population_sel <- declare_population(N = N, 
        u_t1 = rnorm(N) * sd_1, 
        u_t2 = gain_t1t2 + rnorm(N, rho * scale(u_t1), sqrt(1 - rho^2)) * sd_2, 
        Y_t1 = u_t1, 
        L = rnorm(N, r_L.u2 * scale(u_t2), sqrt(1 - r_L.u2^2)) *  sd_2)

```
We then need to add a new *declare_assignment* step to the Data Strategy which selects which participants will be excluded. Because L is a random normal deviate, and children are excluded if they are below average on the Level variable, then we can just exclude those in the intervention group who have a value of L below zero. 

```{r Design5exclude,  warning=F}
# ------------------------------------------------------
# DESIGN 5: SPECIFY EXCLUSIONS
# ------------------------------------------------------
report2 <- declare_assignment(Exclude = (L <0 ) & (Z == 1))   #TRUE (ie exclude) if in intervention group with below avg value on L

```

Finally, we need to modify the Answer Strategy to include the new exclusion.  We name this new answer myancova_sel, to denote that it is an ancova with selection

```{r Design5answer,  warning=F}
# ------------------------------------------------------
# DESIGN 5: SPECIFY ANSWER
# ------------------------------------------------------

ancova_sel <- declare_estimator(Y_t2 ~ Z + Y_t1, .method = lm_robust, 
    inquiry = estimand, subset = (HasData==1 & Exclude==FALSE), label = "Excluding low level group 1")

```
The Model 5 declaration differs from Model 1 in terms of the initial *population_sel* declaration, in the inclusion of the *report2* declaration, which defines the exclusion criteria, and in the use of *ancova_sel* in the answer. All other declarations are carried over from Model 1.

```{r Design5declaration, warning=F}
Design5 <- population_sel + potential_outcomes + 
  estimand + assignment + reveal_t2 + report + report2+
  ancova_sel # model based on Ahmed et al includes report2 to retain only Top-Half GG

designs <- redesign(Design5,r_L.u2=c(0,.2,.4),EffSize=c(0,.2))
Design5_diagnosis<-diagnose_designs(designs,sims=nsims)
save(Design5_diagnosis,file=paste0("simulated_data/Design5_",nsims,".RData"))

```

 
```{r Design5diag,echo=FALSE, warning=FALSE}
ntab<-ntab+1
#we use prior simulated data if we have them
if(haveresults==1){
  load("simulated_data_10K/Design5_10000.RData")
}
tabDes5 <- Design5_diagnosis$diagnosands_df
tabname<-paste0("GGTables/Table ",ntab,"_Diagnostic results from Design 5 at two effect sizes, with different correlations between L and u_t2.csv")
write.csv(tabDes5,tabname,row.names=F)

#simplify table for display
tabDes5a<-tabDes5[,c(2,3,10,11,16,18,20)]
ftab<-flextable(tabDes5a)
ftab<- fit_to_width(ftab,8) #convert from table1 to flextable format to allow formatting of width etc
mycap<-paste0("Table ",ntab,":  Diagnostic results from Design 5 at two effect sizes, with different correlations between L and u_t2")
ftab<-set_caption(ftab,mycap)
ftab
```

Table `r ntab` shows that the selection of cases based on L_u2 creates upwardly biased estimates of the intervention effect. When the null hypothesis is true, the estimates of effect size are .10 and .20 if correlation with the selection measure is .2 or .4 respectively. Note that when the null is true, then 'power' indicates the false positive rate, which should be around .05. The simulated data give values of .17 and .53 when the correlation with selection variable is .2 or .4 respectively. Note also that the power estimates for the real effect of .2 are misleading, because they suggest a strong design, but when we look at the coverage statistic, we see that the estimates are very poor indicators of the true effect.

### Model 6: Combining Designs 4 and 5

In a final set of simulations, we combined Designs 4 and 5 to create simulations that corresponded as closely as possible to Ahmed et al. (2020), to consider the extent of bias introduced by the combination of multiple outcomes and participant selection when the null hypothesis was true. In this case, we assume that the correlation between the selection variable and outcome variables is .2. For each run of this simulation, we select the outcome variable that gives the largest effect size (from the three outcomes that are considered). Figure 2 shows the estimates of effect size that are obtained when the null hypothesis is true, i.e. the correct value of effect size is zero.

```{r compareboth,warning=FALSE}
#We use Design 2 for NFER simulation
#For Ahmed we just change name of the variable used to select cases at the report2 step to match the one we simulated in the multivariate case
report2<-declare_assignment(Exclude = (X.Level.<0)&(Z==1))   #TRUE (ie exclude) if treated with below avg on the simulated Level measure (equivalent to L)
ancovasel_a <- declare_estimator(Y_t2a ~ Z + X.u_t1a., .method = lm_robust, 
                                 inquiry = estimanda, subset = (HasData==1 & Exclude==FALSE), label = "Acov_sela")
ancovasel_b <- declare_estimator(Y_t2b ~ Z + X.u_t1b., .method = lm_robust, 
                                 inquiry = estimandb, subset = (HasData==1 & Exclude==FALSE), label = "Acov_selb")
ancovasel_c <- declare_estimator(Y_t2c ~ Z + X.u_t1c., .method = lm_robust, 
                                 inquiry = estimandc, subset = (HasData==1 & Exclude==FALSE), label = "Acov_selc")

Mod6_Ahmed <- Mod4 + potential_outcomes_a + potential_outcomes_b + potential_outcomes_c + 
  estimanda + estimandb + estimandc + assignment + reveal_t2a + reveal_t2b + reveal_t2c + report + report2+ ancovasel_a + ancovasel_b + ancovasel_c

#NB the value of rho is determined by the correlation matrix used in Mod4, so not needed for the Design

Mod6_designs <- redesign(Mod6_Ahmed,EffSize=c(0)) #just focus on null Design

Mod6_diagnosis<-diagnose_designs(Mod6_designs,sims=nsims)
save(Mod6_diagnosis,file=paste0("simulated_data/Mod6_Ahmedmult_",nsims,".RData"))


if(haveresults==1){

  load("simulated_data_10K/Des2_NFERcl_10000.RData")
   load("simulated_data_10K/Mod6_Ahmedmult_10000.RData")
}
Mod6sims <- Mod6_diagnosis$simulations_df #retain the simulations

Mod6sims$sig<-0 #default
w<-intersect(which(Mod6sims$p.value<.05),which(Mod6sims$estimate>0)) #only consider positive effects
Mod6sims$sig[w]<-1
 
 #We want to count the runs where ANY of outcomes a, b or c is significant
 #Each simulation is identified by sim_id, and there are 6 simulations with that ID; one for each measure and one for each effect size.
 
 Mod6sigs<-aggregate(Mod6sims$sig,by=list(Mod6sims$sim_ID,Mod6sims$EffSize),FUN=sum)
 
Mod6sigstab<-table(Mod6sigs$Group.2,Mod6sigs$x)
Mod6_fprate <- 1-Mod6sigstab[1,1]/max(Mod6sims$sim_ID)


#for plotting comparison with Design 2, select the variable in Model 6 in each simulation with max estimated effect

nsimsx<-max(Mod6sims$sim_ID)
#make new surrogate simulation df with nsims rows
newsim6<-Mod6sims[1:nsimsx,]
for (i in 1:nsimsx){
    w<-which(Mod6sims$sim_ID==i)
    thismax<-which(Mod6sims$estimate[w]==max(Mod6sims$estimate[w])) #find outcome with largest effect
    newsim6[i,]<-Mod6sims[w[thismax],]
}

Des2_NFER_cl_sim<-Des2_NFER_cl_diagnosis$simulations_df
newsim2<-filter(Des2_NFER_cl_sim,rho==.6,estICC==.15,EffSize==0)
w<-which(names(newsim2) %in% c('rho','estICC'))
newsim2<-newsim2[,-w]
newsim2$sig<-0
newsim2$sig[newsim2$p.value<.05]<-1
newsim2$design<-'Model2: NFER_cluster'
newsim6$design<-'Model6: Ahmed'

allsim<-rbind(newsim2,newsim6)



```

**Figure 2**: *Distribution of estimates of effect size for original NFER design (Model 1) and Ahmed et al design (Model 6) when null hypothesis is true; 10,000 simulations.*

It is clear that the combination of using multiple outcomes (model 4) and discarding participants from the intervention group (model 5) dramatically biases the estimates, with a mean estimate of `r mean(newsim6$estimate)` and a false positive rate of `r mean(newsim6$sig)`, even though the amount of correlation between the selection variable and outcome is relatively modest (*r* = .2). The modeling with DeclareDesign helps clarify how two aspects of analysis which individually create some bias in estimates have a much more serious impact when in combination.

```{r plotsims, echo=F}
# function to plot simulations

# NB originally I used geom_histogram with separate subsets for  sig ==0 and sig==1, but what we really want is the whole distribution plotted first, and then a coloured distribution of significant values overlaid. The current code does that, and the odd code where it says 'subset' but then doesn't give a subset for blue plotted histo is just a hangover from earlier code. 

plotsims<-function(myfile,myvar1,myvar2,longvar1,longvar2,mytitle){
  #Altering names of variables to longvar versions to facilitate labeling
c1<-which(names(myfile)==myvar1)
c2<-which(names(myfile)==myvar2)
myfile$myvar1<-paste0(longvar1,myfile[,c1])
myfile$myvar2<-paste0(longvar2,myfile[,c2])

mylabs<-c(myvar1,myvar2)
myg1<-ggplot(myfile,aes(estimate)) + 
    geom_histogram(data=subset(myfile),fill = "blue", alpha = 0.4) +
    geom_histogram(data=subset(myfile,sig == 1),fill = "red", alpha = 0.4) +
    geom_vline(aes(xintercept = estimand),
             lty = "dashed", color = "#C6227F") +
    facet_grid(myvar1~myvar2) +
  labs(x = "Estimate", y = "Count of simulations") +
  ggtitle(mytitle)
return(myg1)
}

#for one dimension only
plotsims2<-function(myfile,myvar1,longvar1,mytitle){
c1<-which(names(myfile)==myvar1)

myfile$myvar1<-paste0(longvar1,myfile[,c1])

mylabs<-c(myvar1)
myg <- ggplot(myfile,aes(estimate)) + 
    geom_histogram(data=subset(myfile),fill = "blue", alpha = 0.4) +
    geom_histogram(data=subset(myfile,sig == 1),fill = "red", alpha = 0.4) +
    geom_vline(aes(xintercept = estimand),
             lty = "dashed", color = "#C6227F") +
    facet_grid(myvar1~.) +
  labs(x = "Estimate", y = "Count of simulations") +
  ggtitle(mytitle)

return(myg)
}

```

```{r dofacetplots, echo=FALSE, warning=FALSE}

 myfile<-allsim
 myvar1<-'design'

 longvar1<-""

 mytitle<-"NFER vs Ahmed design; True Effect size = 0"
 plotGG<-plotsims2(myfile,myvar1,longvar1,mytitle)
 ggsave('simulated_figs/Fig2.png')
 
```

```{r makefirstfig, echo=FALSE,warning=FALSE}
# 
myfile<-Design1_diagnosis$simulations_df
#add flag to show which are significant
myfile$sig<-0
myfile$sig[myfile$p.value<.05]<-1

myvar1<-"rho"
myvar2<-"EffSize"
longvar1<-"reliability (rho): "
longvar2<-"Intervention effect size: "
# 
mytitle <- "Model 1, 10,000 simulations"
plotMod1<-plotsims(myfile,myvar1,myvar2,longvar1,longvar2,mytitle)
ggsave('simulated_figs/FigModel1.png')

```

## Discussion

The points made by these simulations are not novel. The importance of using randomisation to evaluate interventions, and of preregistering the analysis and adjusting statistical tests for number of outcomes have been recognised for decades. However, we suspect that many researchers lack an intuitive sense of how serious the consequences may be if these basic rules are broken.

As demonstrated in Model 5, when randomisation is broken and we select participants to include based on a variable (called here L, the level reached on GraphoGame) we introduce bias, or a systematic distortion of the true effect size. The degree of bias varies systematically with the extent to which L correlates with the level of reading ability at time 2. The higher that correlation, the greater the more the estimated effect size is (artifactually) increased. When the correlation is zero we obtain an unbiased (accurate) effect size. As the positive correlation gets larger, the estimated effect size gets larger.

It is important to emphasise that such correlations are almost always likely to occur, which is why conducting randomised experiments is so important -- because by definition in a randomised experiment we "break" any correlation that exists between participants' characteristics (in this case their propensity to learn to read) and the treatment they receive (GraphoGame versus no extra help). This basic idea has been accepted for centuries (Fisher and before...Bacon???) and it is why randomised experiments are so powerful. If you want to decide if an educational intervention works, if you can, do an randomised experiment to test its effects. If a randomise experiment shows a null result, any post hoc analyses of the data are fraught with difficulties. Avoid them at all costs, except as a prelude to another, randomised experiment.

We argue here that the conclusions from the analysis by Ahmed et al are insecure, because they used methods that introduce bias into estimates of effects of intervention - selection of subgroups for analysis after the study is completed. The problems created by such methods are well-known in fields such as clinical trials and political science. For instance, in their Guideline on the investigation of subgroups in confirmatory clinical trials, the Committee for Medicinal Products for Human Use (2019) stated: "From a formal statistical point of view, no further confirmatory conclusions are possible in a clinical trial where the primary null hypothesis cannot be rejected." And in a paper entitled: "How conditioning on post-treatment variables can ruin your experiment and what to do about it", Montgomery et al (2018) noted the dangers of practices such as "dropping participants who fail manipulation checks; controlling for variables measured after the treatment such as potential mediators; or subsetting samples based on post-treatment variables". All of these practices can lead to biased estimates. Unfortunately, the analyses conducted by Ahmed et al (2020) fall in this category. Here we use simulations to show how, under certain reasonable assumptions, the methods that they used are likely to have inflated the estimate of the intervention effect.

This problem is compounded if in addition multiple outcomes are considered without adequate statistical correction. Bishop (2023) compared different approaches to handling this situation, noting that the popular method of applying a Bonferroni correction is over-conservative when outcome measures are correlated. Taking a single principal component as the dependent variable is a good way of achieving optimal statistical power without increasing the false positive rate when a suite of outcome measures are positively correlated and may be reasonably assumed to measure a common underlying construct. An alternative approach, MEff, is similar to Bonferroni correction, but adjusts for intercorrelations between measures, and may be used when a set of outcome measures is more heterogeneous.

# References

Ahmed, H., Wilson, A., Mead, N., Noble, H., Richardson, U., Wolpert, M. A., & Goswami, U. (2020). An evaluation of the efficacy of Graphogame Rime for promoting English phonics knowledge in poor readers. Frontiers in Education, 5. <https://www.frontiersin.org/articles/10.3389/feduc.2020.00132>

Blair, G., Cooper, J., Coppock, A., & Humphreys, M. (2019). Declaring and diagnosing research designs. *American Political Science Review*, *113*(3), 838--859. <https://doi.org/10.1017/S0003055419000194>

Blair, G., Coppock, A., & Humphreys, M. (2023). Research Design in the Social Sciences: Declaration, Diagnosis, and Redesign. Princeton University Press. <https://book.declaredesign.org>

Goswami, U., & East, M. (2000). Rhyme and analogy in beginning reading: Conceptual and methodological issues. Applied Psycholinguistics, 21(1), 63--93. <https://doi.org/10.1017/S0142716400001041>

Kraemer, H. C., Gardner, C., Brooks III, J. O., & Yesavage, J. A. (1998). Advantages of excluding underpowered studies in meta-analysis: Inclusionist versus exclusionist viewpoints. *Psychological Methods*, *3*(1), 23--31. <https://doi.org/10.1037/1082-989X.3.1.23>

McTigue, E. M., Solheim, O. J., Zimmer, W. K., & Uppstad, P. H. (2019). Critically reviewing graphogame across the world: Recommendations and cautions for research and implementation of computer‐assisted instruction for word‐reading acquisition. Reading Research Quarterly, 55(1), 45--73. <https://doi.org/doi:10.1002/rrq.256>

Schweinsberg, M., Feldman, M., Staub, N., van den Akker, O. R., van Aert, R. C. M., van Assen, M. A. L. M., Liu, Y., Althoff, T., Heer, J., Kale, A., Mohamed, Z., Amireh, H., Venkatesh Prasad, V., Bernstein, A., Robinson, E., Snellman, K., Amy Sommer, S., Otner, S. M. G., Robinson, D., ... Luis Uhlmann, E. (2021). Same data, different conclusions: Radical dispersion in empirical results when independent analysts operationalize and test the same hypothesis. Organizational Behavior and Human Decision Processes, 165, 228--249. <https://doi.org/10.1016/j.obhdp.2021.02.003>

Shadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal influence. Houghton, Mifflin and Company.

Worth, J., Nelson, J., Harland, J., Bernardinelli, D., & Styles, B. (2018). GraphoGame Rime: Evaluation report and executive summary. National Foundation for Educational Research. <https://www.nfer.ac.uk/publications/graphogame-rime-evaluation-report-and-executive-summary/>
